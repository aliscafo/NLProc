{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Untitled.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6eE2Kn1RZrR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from fastai import *\n",
        "from fastai.basic_data import *\n",
        "from fastai.text import *\n",
        "from torch import *\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoZzfk7nRlZ6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "20606966-9887-4e74-b326-441b65efaf83"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  print(\"True\")"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YYw1LMGRZri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('texts_train.txt', 'r') as file_texts:\n",
        "    texts = file_texts.read().split(\"\\n\")\n",
        "file_texts.close()\n",
        "\n",
        "with open('scores_train.txt', 'r') as file_scores:\n",
        "    scores = file_scores.read().split(\"\\n\")\n",
        "file_scores.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvblTLgWRZsB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts_train, texts_val, scores_train, scores_val = train_test_split(texts, scores, train_size=0.85)\n",
        "\n",
        "d_train = {'review_content': texts_train, 'rating': scores_train}\n",
        "training_df = pd.DataFrame(data=d_train)\n",
        "\n",
        "d_val = {'review_content': texts_val, 'rating': scores_val}\n",
        "validation_df = pd.DataFrame(data=d_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvDPC7eVRZsJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "351be408-f2aa-4ba4-a048-ac197ca32fde"
      },
      "source": [
        "validation_df"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_content</th>\n",
              "      <th>rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>..когда-то в далеком 93 я купил две видеокассе...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Отличный образчик боевой фантастики. Особенно ...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Потроясающе. Юмор, владение сюжетной линией. А...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Самая любимая книга у Коэльо. Паоло Коэльо как...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Нормально. Но желания сразу перечитать не возн...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2996</th>\n",
              "      <td>мои первые шаги в осознанность начинались вот ...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2997</th>\n",
              "      <td>Кто знает об авторе, какие-нибудь сведения. Я ...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2998</th>\n",
              "      <td>Как и ожидалось, книга захватывает. Герой заст...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2999</th>\n",
              "      <td>Надо было назвать этот фильм 4 идиота. А если ...</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3000</th>\n",
              "      <td>Закручено замечательно (Панов мастер придумыва...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3001 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         review_content rating\n",
              "0     ..когда-то в далеком 93 я купил две видеокассе...     10\n",
              "1     Отличный образчик боевой фантастики. Особенно ...      9\n",
              "2     Потроясающе. Юмор, владение сюжетной линией. А...      9\n",
              "3     Самая любимая книга у Коэльо. Паоло Коэльо как...      9\n",
              "4     Нормально. Но желания сразу перечитать не возн...      7\n",
              "...                                                 ...    ...\n",
              "2996  мои первые шаги в осознанность начинались вот ...     10\n",
              "2997  Кто знает об авторе, какие-нибудь сведения. Я ...     10\n",
              "2998  Как и ожидалось, книга захватывает. Герой заст...      9\n",
              "2999  Надо было назвать этот фильм 4 идиота. А если ...      8\n",
              "3000  Закручено замечательно (Панов мастер придумыва...      5\n",
              "\n",
              "[3001 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msVv9i9dRZsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('dataset_40757_1.txt', 'r') as file_tests:\n",
        "    texts_test = file_tests.read().strip(\"\\n\").split(\"\\n\")\n",
        "file_tests.close()\n",
        "\n",
        "d_test = {'review_content': texts_test}\n",
        "test_df = pd.DataFrame(data=d_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gxtenUGRZsX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "4b80e889-5c58-42b7-925b-9d28a4e25c9e"
      },
      "source": [
        "test_df"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ну чото там все понятно было сразу.не очень ин...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Мне книга понравилась - очень - этакая смесь м...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>В отношении книги меня можно записать в число ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>нет определённого мнения насчёт этого фильма. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Тех кто считает, что «войны будут всегда, пока...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>фильм показаллся устарелым: игра актеров, общи...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>Я таки \"добила\" третью часть \"Берсерков\". Ради...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>Фэповско-нашистская подстава. И подмена. Делиш...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>абсолютно двоякое чувство к этому фильму. хоче...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>Действительно, книга интересная и легко читает...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        review_content\n",
              "0    ну чото там все понятно было сразу.не очень ин...\n",
              "1    Мне книга понравилась - очень - этакая смесь м...\n",
              "2    В отношении книги меня можно записать в число ...\n",
              "3    нет определённого мнения насчёт этого фильма. ...\n",
              "4    Тех кто считает, что «войны будут всегда, пока...\n",
              "..                                                 ...\n",
              "995  фильм показаллся устарелым: игра актеров, общи...\n",
              "996  Я таки \"добила\" третью часть \"Берсерков\". Ради...\n",
              "997  Фэповско-нашистская подстава. И подмена. Делиш...\n",
              "998  абсолютно двоякое чувство к этому фильму. хоче...\n",
              "999  Действительно, книга интересная и легко читает...\n",
              "\n",
              "[1000 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3r3XG3w5RZse",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path =  Path('data')\n",
        "\n",
        "# Language model data\n",
        "data_lm = TextLMDataBunch.from_df(\n",
        "    path,\n",
        "    training_df, \n",
        "    valid_df=validation_df, \n",
        "    test_df=test_df, \n",
        "    text_cols=['review_content'], \n",
        "    label_cols=['rating'])\n",
        "\n",
        "# Classifier model data\n",
        "data_clas = TextClasDataBunch.from_df(\n",
        "    path, \n",
        "    train_df=training_df, \n",
        "    valid_df=validation_df, \n",
        "    test_df=test_df, \n",
        "    vocab=data_lm.train_ds.vocab, \n",
        "    text_cols=['review_content'], \n",
        "    label_cols=['rating'], bs=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW6mHNViRZsl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_lm.save('data_lm_export.pkl')\n",
        "data_clas.save('data_clas_export.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfTvTuJpRZsp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_lm = load_data(path, 'data_lm_export.pkl')\n",
        "data_clas = load_data(path, 'data_clas_export.pkl', bs=16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRI-qX10RZs0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OEILnt_RZs6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "b4222865-2a79-4917-c2de-b2a9e913a391"
      },
      "source": [
        "learn.fit_one_cycle(1, 1e-2)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>5.928420</td>\n",
              "      <td>5.423859</td>\n",
              "      <td>0.228155</td>\n",
              "      <td>02:34</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlOq4AfIn5oB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crfhfFrURZtA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "ed0155bb-7ce9-49fe-d30c-9f821894170f"
      },
      "source": [
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(1, 1e-2)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>5.288733</td>\n",
              "      <td>4.911573</td>\n",
              "      <td>0.267385</td>\n",
              "      <td>02:58</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NZ7-cmnUvhg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.save_encoder('ft_enc')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TD7B0rULU920",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a02aada7-9695-49d6-bea9-934ecce6bafd"
      },
      "source": [
        "learn.predict('да', n_words=10)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'да смешно . мне понравилось , но как будто стоит было'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_AtwslIVFr1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f12a5ca4-2cce-42d5-ad38-2a2eef6c087b"
      },
      "source": [
        "learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5)\n",
        "# Load the language model we just trained as encoder\n",
        "learn.load_encoder('ft_enc')"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNLearner(data=TextClasDataBunch;\n",
              "\n",
              "Train: LabelList (17000 items)\n",
              "x: TextList\n",
              "xxbos xxmaj лучшая вещь у xxmaj акунина . xxmaj явно выделяется из прочих , которые , по - моему , стали чересчур похожи друг на друга . xxmaj из новелл лучшая , кажется , об английском кладбище .,xxbos xxmaj отличная книга ! xxmaj хотя я и не увлекаюсь фентези , но тут что - то меня xxunk прочитать эту книгу по многочисленным рекомендациям клуба . xxmaj не разачарована . xxmaj как пишут многие , действительно было искренне жаль дочитывать . xxmaj обычно я xxunk прочесть поскорее , узнать развязку , но тут с самого начала было ясно что добро победит зло :) и я наслаждалась слогом , юмором , доброй сказкой :) xxmaj хорошо что книги есть продолжения :) xxmaj замечательно,xxbos книга не произвела xxunk,xxbos xxmaj не ожидала от xxmaj xxunk такой слабой вещи . xxmaj идея отличная - клинки и все такое , но читала с xxunk ... xxmaj хотя , может это перевод такой дурацкий .,xxbos xxmaj xxunk супер личность . xxmaj но , это далеко не лучшая книга о нем .\n",
              "y: CategoryList\n",
              "9,9,3,6,4\n",
              "Path: data;\n",
              "\n",
              "Valid: LabelList (3000 items)\n",
              "x: TextList\n",
              "xxbos .. когда - то в далеком xxunk я купил две xxunk xxup городок и xxup поездка в xxup америку с xxup эдди xxup xxunk и хотя у меня у меня не было xxunk , я положил их на самом видном месте ... мои две главные духовные xxunk на тот довольно серьезный момент мой жизни ... ... и воспринимаю xxup xxunk и xxup xxunk не как xxunk , а как представителей духовного сопротивления ... ... таких как xxup xxunk , xxup бодров , xxup пелевин ...,xxbos xxmaj отличный образчик боевой фантастики . xxmaj особенно понравилась детальная проработка персонажей , а также продуманная модель боевых действий в космосе . а то умиляют авторы которые тупо берут скажем морскую xxunk и тупо xxunk в космос .,xxbos xxmaj xxunk . xxmaj юмор , владение сюжетной линией . а техника ! xxmaj xxunk рифмы , авторские xxunk ... xxmaj профессионально , легко - по - настоящему здорово .,xxbos xxmaj самая любимая книга у xxmaj коэльо . xxmaj паоло xxmaj коэльо как всегда - говорит просто , а думать заставляет . xxmaj обязательно прочитайте , оно того стоит .,xxbos xxmaj нормально . xxmaj но желания сразу перечитать не возникло . xxmaj отдельные моменты из книги xxunk .\n",
              "y: CategoryList\n",
              "10,9,9,9,7\n",
              "Path: data;\n",
              "\n",
              "Test: LabelList (1000 items)\n",
              "x: TextList\n",
              "xxbos ну xxunk там все понятно было xxunk очень xxunk красиво .,xxbos xxmaj мне книга понравилась - очень - этакая смесь мистики , приключений и любовного романа . xxmaj очень легко и интересно написана - лично мне оторваться было сложно .,xxbos в отношении книги меня можно записать в число фанатов . а фильм - попса .,xxbos нет определённого мнения насчёт этого фильма . вроде и не понравился , но и нет xxunk возражения против него . на один раз .,xxbos xxmaj тех кто считает , что « войны будут всегда , пока будут находиться люди готовые умирать » xxunk делает xxunk .\n",
              "y: EmptyLabelList\n",
              ",,,,\n",
              "Path: data, model=SequentialRNN(\n",
              "  (0): MultiBatchEncoder(\n",
              "    (module): AWD_LSTM(\n",
              "      (encoder): Embedding(41784, 400, padding_idx=1)\n",
              "      (encoder_dp): EmbeddingDropout(\n",
              "        (emb): Embedding(41784, 400, padding_idx=1)\n",
              "      )\n",
              "      (rnns): ModuleList(\n",
              "        (0): WeightDropout(\n",
              "          (module): LSTM(400, 1152, batch_first=True)\n",
              "        )\n",
              "        (1): WeightDropout(\n",
              "          (module): LSTM(1152, 1152, batch_first=True)\n",
              "        )\n",
              "        (2): WeightDropout(\n",
              "          (module): LSTM(1152, 400, batch_first=True)\n",
              "        )\n",
              "      )\n",
              "      (input_dp): RNNDropout()\n",
              "      (hidden_dps): ModuleList(\n",
              "        (0): RNNDropout()\n",
              "        (1): RNNDropout()\n",
              "        (2): RNNDropout()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (1): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=10, bias=True)\n",
              "    )\n",
              "  )\n",
              "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f7aa2a77f28>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('data'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
              "learn: RNNLearner(data=TextClasDataBunch;\n",
              "\n",
              "Train: LabelList (17000 items)\n",
              "x: TextList\n",
              "xxbos xxmaj лучшая вещь у xxmaj акунина . xxmaj явно выделяется из прочих , которые , по - моему , стали чересчур похожи друг на друга . xxmaj из новелл лучшая , кажется , об английском кладбище .,xxbos xxmaj отличная книга ! xxmaj хотя я и не увлекаюсь фентези , но тут что - то меня xxunk прочитать эту книгу по многочисленным рекомендациям клуба . xxmaj не разачарована . xxmaj как пишут многие , действительно было искренне жаль дочитывать . xxmaj обычно я xxunk прочесть поскорее , узнать развязку , но тут с самого начала было ясно что добро победит зло :) и я наслаждалась слогом , юмором , доброй сказкой :) xxmaj хорошо что книги есть продолжения :) xxmaj замечательно,xxbos книга не произвела xxunk,xxbos xxmaj не ожидала от xxmaj xxunk такой слабой вещи . xxmaj идея отличная - клинки и все такое , но читала с xxunk ... xxmaj хотя , может это перевод такой дурацкий .,xxbos xxmaj xxunk супер личность . xxmaj но , это далеко не лучшая книга о нем .\n",
              "y: CategoryList\n",
              "9,9,3,6,4\n",
              "Path: data;\n",
              "\n",
              "Valid: LabelList (3000 items)\n",
              "x: TextList\n",
              "xxbos .. когда - то в далеком xxunk я купил две xxunk xxup городок и xxup поездка в xxup америку с xxup эдди xxup xxunk и хотя у меня у меня не было xxunk , я положил их на самом видном месте ... мои две главные духовные xxunk на тот довольно серьезный момент мой жизни ... ... и воспринимаю xxup xxunk и xxup xxunk не как xxunk , а как представителей духовного сопротивления ... ... таких как xxup xxunk , xxup бодров , xxup пелевин ...,xxbos xxmaj отличный образчик боевой фантастики . xxmaj особенно понравилась детальная проработка персонажей , а также продуманная модель боевых действий в космосе . а то умиляют авторы которые тупо берут скажем морскую xxunk и тупо xxunk в космос .,xxbos xxmaj xxunk . xxmaj юмор , владение сюжетной линией . а техника ! xxmaj xxunk рифмы , авторские xxunk ... xxmaj профессионально , легко - по - настоящему здорово .,xxbos xxmaj самая любимая книга у xxmaj коэльо . xxmaj паоло xxmaj коэльо как всегда - говорит просто , а думать заставляет . xxmaj обязательно прочитайте , оно того стоит .,xxbos xxmaj нормально . xxmaj но желания сразу перечитать не возникло . xxmaj отдельные моменты из книги xxunk .\n",
              "y: CategoryList\n",
              "10,9,9,9,7\n",
              "Path: data;\n",
              "\n",
              "Test: LabelList (1000 items)\n",
              "x: TextList\n",
              "xxbos ну xxunk там все понятно было xxunk очень xxunk красиво .,xxbos xxmaj мне книга понравилась - очень - этакая смесь мистики , приключений и любовного романа . xxmaj очень легко и интересно написана - лично мне оторваться было сложно .,xxbos в отношении книги меня можно записать в число фанатов . а фильм - попса .,xxbos нет определённого мнения насчёт этого фильма . вроде и не понравился , но и нет xxunk возражения против него . на один раз .,xxbos xxmaj тех кто считает , что « войны будут всегда , пока будут находиться люди готовые умирать » xxunk делает xxunk .\n",
              "y: EmptyLabelList\n",
              ",,,,\n",
              "Path: data, model=SequentialRNN(\n",
              "  (0): MultiBatchEncoder(\n",
              "    (module): AWD_LSTM(\n",
              "      (encoder): Embedding(41784, 400, padding_idx=1)\n",
              "      (encoder_dp): EmbeddingDropout(\n",
              "        (emb): Embedding(41784, 400, padding_idx=1)\n",
              "      )\n",
              "      (rnns): ModuleList(\n",
              "        (0): WeightDropout(\n",
              "          (module): LSTM(400, 1152, batch_first=True)\n",
              "        )\n",
              "        (1): WeightDropout(\n",
              "          (module): LSTM(1152, 1152, batch_first=True)\n",
              "        )\n",
              "        (2): WeightDropout(\n",
              "          (module): LSTM(1152, 400, batch_first=True)\n",
              "        )\n",
              "      )\n",
              "      (input_dp): RNNDropout()\n",
              "      (hidden_dps): ModuleList(\n",
              "        (0): RNNDropout()\n",
              "        (1): RNNDropout()\n",
              "        (2): RNNDropout()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (1): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=10, bias=True)\n",
              "    )\n",
              "  )\n",
              "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f7aa2a77f28>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('data'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[...], layer_groups=[Sequential(\n",
              "  (0): Embedding(41784, 400, padding_idx=1)\n",
              "  (1): EmbeddingDropout(\n",
              "    (emb): Embedding(41784, 400, padding_idx=1)\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(400, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 400, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=10, bias=True)\n",
              "    )\n",
              "  )\n",
              ")], add_time=True, silent=False)\n",
              "alpha: 2.0\n",
              "beta: 1.0], layer_groups=[Sequential(\n",
              "  (0): Embedding(41784, 400, padding_idx=1)\n",
              "  (1): EmbeddingDropout(\n",
              "    (emb): Embedding(41784, 400, padding_idx=1)\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(400, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 400, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=10, bias=True)\n",
              "    )\n",
              "  )\n",
              ")], add_time=True, silent=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GHCPlhTVQiR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "0ccf8ecd-3535-4369-a361-92981c3fcad7"
      },
      "source": [
        "data_clas.show_batch()"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>xxbos xxmaj посмотрел тут \" 12 \" xxmaj михалкова . xxmaj начало просто завораживающее , с самогон начала на весь экран - отличная фраза \" xxmaj не пытайтесь найти правду быта , пытайтесь найти правду бытия \" ( тут же записал ) , снято отлично , прямо xxunk в экран , потом эти двенадцать мужиков , как они в xxunk xxunk - в общем , первые 20 минут наверное .</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxbos xxmaj три брата – флегматик ( xxmaj xxunk ) , меланхолик ( xxmaj броуди ) и холерик ( xxmaj уилсон ) , – не xxunk год после гибели отца , отправляются в путешествие по xxmaj индии в вагоне первого класса в компании с xxunk розовыми чемоданами , xxunk , аппаратом для ламинирования и секретарем в очках для поддержания xxunk дня . xxmaj цель поездки – xxunk семейных xxunk и</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxbos xxmaj роман xxmaj маргарет xxmaj митчел приоткрывает нам дверь в американское общество времен xxmaj гражданской войны . xxmaj войны xxmaj севера и xxmaj юга . xxmaj страшной войны . xxmaj ведь войны внутри страны самые страшные . xxmaj на фоне этих ужасных событий и разворачивается действие этого романа . xxmaj автор знакомит нас с очаровательной девушкой xxmaj скарлетт о\"хара , забавная папина любимица , которая всегда получает то ,</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxbos xxmaj однажды в прессу xxunk высказывание xxmaj чака xxmaj норриса , xxunk в порядке общей xxunk , о том , что из него такой же актёр , как из xxmaj дэвида xxmaj xxunk – спортсмен . и тем не менее встреча двух « звёзд » фильмов действия – верных последователей дела xxmaj брюса xxmaj ли , xxunk первую известность благодаря проектам , к которым был так или иначе причастен</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxbos xxmaj если отобрать всё самое странное и xxunk , что только было в « трилогии xxunk , доведя это до наивысшей степени концентрации , – как раз получится очень необычное произведение со столь xxunk и xxunk названием . xxmaj горько xxunk в своих ожиданиях те , кто xxunk на восприятие научно - фантастической комедии . xxmaj или даже мрачной xxunk , в которой xxunk фантазия художника - визионера соседствует</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7O4nFFJGVXoN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "f9c2adde-c8d7-4023-c060-94d938e86805"
      },
      "source": [
        "learn.fit_one_cycle(1, 1e-2)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.831686</td>\n",
              "      <td>1.764436</td>\n",
              "      <td>0.331000</td>\n",
              "      <td>02:46</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExzISDUrVYLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save intermediate model to enable resuming training from stages\n",
        "learn.save('stage-1-model.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxmihyFFWPQM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4078a633-941c-4525-ca60-2bd36c17f33c"
      },
      "source": [
        "learn.load('stage-1-model.pkl')"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNLearner(data=TextClasDataBunch;\n",
              "\n",
              "Train: LabelList (17000 items)\n",
              "x: TextList\n",
              "xxbos xxmaj лучшая вещь у xxmaj акунина . xxmaj явно выделяется из прочих , которые , по - моему , стали чересчур похожи друг на друга . xxmaj из новелл лучшая , кажется , об английском кладбище .,xxbos xxmaj отличная книга ! xxmaj хотя я и не увлекаюсь фентези , но тут что - то меня xxunk прочитать эту книгу по многочисленным рекомендациям клуба . xxmaj не разачарована . xxmaj как пишут многие , действительно было искренне жаль дочитывать . xxmaj обычно я xxunk прочесть поскорее , узнать развязку , но тут с самого начала было ясно что добро победит зло :) и я наслаждалась слогом , юмором , доброй сказкой :) xxmaj хорошо что книги есть продолжения :) xxmaj замечательно,xxbos книга не произвела xxunk,xxbos xxmaj не ожидала от xxmaj xxunk такой слабой вещи . xxmaj идея отличная - клинки и все такое , но читала с xxunk ... xxmaj хотя , может это перевод такой дурацкий .,xxbos xxmaj xxunk супер личность . xxmaj но , это далеко не лучшая книга о нем .\n",
              "y: CategoryList\n",
              "9,9,3,6,4\n",
              "Path: data;\n",
              "\n",
              "Valid: LabelList (3000 items)\n",
              "x: TextList\n",
              "xxbos .. когда - то в далеком xxunk я купил две xxunk xxup городок и xxup поездка в xxup америку с xxup эдди xxup xxunk и хотя у меня у меня не было xxunk , я положил их на самом видном месте ... мои две главные духовные xxunk на тот довольно серьезный момент мой жизни ... ... и воспринимаю xxup xxunk и xxup xxunk не как xxunk , а как представителей духовного сопротивления ... ... таких как xxup xxunk , xxup бодров , xxup пелевин ...,xxbos xxmaj отличный образчик боевой фантастики . xxmaj особенно понравилась детальная проработка персонажей , а также продуманная модель боевых действий в космосе . а то умиляют авторы которые тупо берут скажем морскую xxunk и тупо xxunk в космос .,xxbos xxmaj xxunk . xxmaj юмор , владение сюжетной линией . а техника ! xxmaj xxunk рифмы , авторские xxunk ... xxmaj профессионально , легко - по - настоящему здорово .,xxbos xxmaj самая любимая книга у xxmaj коэльо . xxmaj паоло xxmaj коэльо как всегда - говорит просто , а думать заставляет . xxmaj обязательно прочитайте , оно того стоит .,xxbos xxmaj нормально . xxmaj но желания сразу перечитать не возникло . xxmaj отдельные моменты из книги xxunk .\n",
              "y: CategoryList\n",
              "10,9,9,9,7\n",
              "Path: data;\n",
              "\n",
              "Test: LabelList (1000 items)\n",
              "x: TextList\n",
              "xxbos ну xxunk там все понятно было xxunk очень xxunk красиво .,xxbos xxmaj мне книга понравилась - очень - этакая смесь мистики , приключений и любовного романа . xxmaj очень легко и интересно написана - лично мне оторваться было сложно .,xxbos в отношении книги меня можно записать в число фанатов . а фильм - попса .,xxbos нет определённого мнения насчёт этого фильма . вроде и не понравился , но и нет xxunk возражения против него . на один раз .,xxbos xxmaj тех кто считает , что « войны будут всегда , пока будут находиться люди готовые умирать » xxunk делает xxunk .\n",
              "y: EmptyLabelList\n",
              ",,,,\n",
              "Path: data, model=SequentialRNN(\n",
              "  (0): MultiBatchEncoder(\n",
              "    (module): AWD_LSTM(\n",
              "      (encoder): Embedding(41784, 400, padding_idx=1)\n",
              "      (encoder_dp): EmbeddingDropout(\n",
              "        (emb): Embedding(41784, 400, padding_idx=1)\n",
              "      )\n",
              "      (rnns): ModuleList(\n",
              "        (0): WeightDropout(\n",
              "          (module): LSTM(400, 1152, batch_first=True)\n",
              "        )\n",
              "        (1): WeightDropout(\n",
              "          (module): LSTM(1152, 1152, batch_first=True)\n",
              "        )\n",
              "        (2): WeightDropout(\n",
              "          (module): LSTM(1152, 400, batch_first=True)\n",
              "        )\n",
              "      )\n",
              "      (input_dp): RNNDropout()\n",
              "      (hidden_dps): ModuleList(\n",
              "        (0): RNNDropout()\n",
              "        (1): RNNDropout()\n",
              "        (2): RNNDropout()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (1): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=10, bias=True)\n",
              "    )\n",
              "  )\n",
              "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f7aa2a77f28>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('data'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
              "learn: RNNLearner(data=TextClasDataBunch;\n",
              "\n",
              "Train: LabelList (17000 items)\n",
              "x: TextList\n",
              "xxbos xxmaj лучшая вещь у xxmaj акунина . xxmaj явно выделяется из прочих , которые , по - моему , стали чересчур похожи друг на друга . xxmaj из новелл лучшая , кажется , об английском кладбище .,xxbos xxmaj отличная книга ! xxmaj хотя я и не увлекаюсь фентези , но тут что - то меня xxunk прочитать эту книгу по многочисленным рекомендациям клуба . xxmaj не разачарована . xxmaj как пишут многие , действительно было искренне жаль дочитывать . xxmaj обычно я xxunk прочесть поскорее , узнать развязку , но тут с самого начала было ясно что добро победит зло :) и я наслаждалась слогом , юмором , доброй сказкой :) xxmaj хорошо что книги есть продолжения :) xxmaj замечательно,xxbos книга не произвела xxunk,xxbos xxmaj не ожидала от xxmaj xxunk такой слабой вещи . xxmaj идея отличная - клинки и все такое , но читала с xxunk ... xxmaj хотя , может это перевод такой дурацкий .,xxbos xxmaj xxunk супер личность . xxmaj но , это далеко не лучшая книга о нем .\n",
              "y: CategoryList\n",
              "9,9,3,6,4\n",
              "Path: data;\n",
              "\n",
              "Valid: LabelList (3000 items)\n",
              "x: TextList\n",
              "xxbos .. когда - то в далеком xxunk я купил две xxunk xxup городок и xxup поездка в xxup америку с xxup эдди xxup xxunk и хотя у меня у меня не было xxunk , я положил их на самом видном месте ... мои две главные духовные xxunk на тот довольно серьезный момент мой жизни ... ... и воспринимаю xxup xxunk и xxup xxunk не как xxunk , а как представителей духовного сопротивления ... ... таких как xxup xxunk , xxup бодров , xxup пелевин ...,xxbos xxmaj отличный образчик боевой фантастики . xxmaj особенно понравилась детальная проработка персонажей , а также продуманная модель боевых действий в космосе . а то умиляют авторы которые тупо берут скажем морскую xxunk и тупо xxunk в космос .,xxbos xxmaj xxunk . xxmaj юмор , владение сюжетной линией . а техника ! xxmaj xxunk рифмы , авторские xxunk ... xxmaj профессионально , легко - по - настоящему здорово .,xxbos xxmaj самая любимая книга у xxmaj коэльо . xxmaj паоло xxmaj коэльо как всегда - говорит просто , а думать заставляет . xxmaj обязательно прочитайте , оно того стоит .,xxbos xxmaj нормально . xxmaj но желания сразу перечитать не возникло . xxmaj отдельные моменты из книги xxunk .\n",
              "y: CategoryList\n",
              "10,9,9,9,7\n",
              "Path: data;\n",
              "\n",
              "Test: LabelList (1000 items)\n",
              "x: TextList\n",
              "xxbos ну xxunk там все понятно было xxunk очень xxunk красиво .,xxbos xxmaj мне книга понравилась - очень - этакая смесь мистики , приключений и любовного романа . xxmaj очень легко и интересно написана - лично мне оторваться было сложно .,xxbos в отношении книги меня можно записать в число фанатов . а фильм - попса .,xxbos нет определённого мнения насчёт этого фильма . вроде и не понравился , но и нет xxunk возражения против него . на один раз .,xxbos xxmaj тех кто считает , что « войны будут всегда , пока будут находиться люди готовые умирать » xxunk делает xxunk .\n",
              "y: EmptyLabelList\n",
              ",,,,\n",
              "Path: data, model=SequentialRNN(\n",
              "  (0): MultiBatchEncoder(\n",
              "    (module): AWD_LSTM(\n",
              "      (encoder): Embedding(41784, 400, padding_idx=1)\n",
              "      (encoder_dp): EmbeddingDropout(\n",
              "        (emb): Embedding(41784, 400, padding_idx=1)\n",
              "      )\n",
              "      (rnns): ModuleList(\n",
              "        (0): WeightDropout(\n",
              "          (module): LSTM(400, 1152, batch_first=True)\n",
              "        )\n",
              "        (1): WeightDropout(\n",
              "          (module): LSTM(1152, 1152, batch_first=True)\n",
              "        )\n",
              "        (2): WeightDropout(\n",
              "          (module): LSTM(1152, 400, batch_first=True)\n",
              "        )\n",
              "      )\n",
              "      (input_dp): RNNDropout()\n",
              "      (hidden_dps): ModuleList(\n",
              "        (0): RNNDropout()\n",
              "        (1): RNNDropout()\n",
              "        (2): RNNDropout()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (1): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=10, bias=True)\n",
              "    )\n",
              "  )\n",
              "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f7aa2a77f28>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('data'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[...], layer_groups=[Sequential(\n",
              "  (0): Embedding(41784, 400, padding_idx=1)\n",
              "  (1): EmbeddingDropout(\n",
              "    (emb): Embedding(41784, 400, padding_idx=1)\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(400, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 400, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=10, bias=True)\n",
              "    )\n",
              "  )\n",
              ")], add_time=True, silent=False)\n",
              "alpha: 2.0\n",
              "beta: 1.0], layer_groups=[Sequential(\n",
              "  (0): Embedding(41784, 400, padding_idx=1)\n",
              "  (1): EmbeddingDropout(\n",
              "    (emb): Embedding(41784, 400, padding_idx=1)\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(400, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 400, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=10, bias=True)\n",
              "    )\n",
              "  )\n",
              ")], add_time=True, silent=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BVFFCEuWP5N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "97ba4568-b117-4ed6-d242-5f9dfc574fda"
      },
      "source": [
        "learn.freeze_to(-2)\n",
        "# Slice up the learning rates to apply different rates to different layers of the network - the deeper the larger.\n",
        "learn.fit_one_cycle(1, slice(5e-3/2., 5e-3))"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.741259</td>\n",
              "      <td>1.672128</td>\n",
              "      <td>0.352000</td>\n",
              "      <td>03:36</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwl0pO7wXGKd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.save('stage-2-model.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD5BgnUbXGeX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "71bbc705-9903-45d6-bcf6-f9a9a4a55c11"
      },
      "source": [
        "learn.load('stage-2-model.pkl')"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNLearner(data=TextClasDataBunch;\n",
              "\n",
              "Train: LabelList (17000 items)\n",
              "x: TextList\n",
              "xxbos xxmaj лучшая вещь у xxmaj акунина . xxmaj явно выделяется из прочих , которые , по - моему , стали чересчур похожи друг на друга . xxmaj из новелл лучшая , кажется , об английском кладбище .,xxbos xxmaj отличная книга ! xxmaj хотя я и не увлекаюсь фентези , но тут что - то меня xxunk прочитать эту книгу по многочисленным рекомендациям клуба . xxmaj не разачарована . xxmaj как пишут многие , действительно было искренне жаль дочитывать . xxmaj обычно я xxunk прочесть поскорее , узнать развязку , но тут с самого начала было ясно что добро победит зло :) и я наслаждалась слогом , юмором , доброй сказкой :) xxmaj хорошо что книги есть продолжения :) xxmaj замечательно,xxbos книга не произвела xxunk,xxbos xxmaj не ожидала от xxmaj xxunk такой слабой вещи . xxmaj идея отличная - клинки и все такое , но читала с xxunk ... xxmaj хотя , может это перевод такой дурацкий .,xxbos xxmaj xxunk супер личность . xxmaj но , это далеко не лучшая книга о нем .\n",
              "y: CategoryList\n",
              "9,9,3,6,4\n",
              "Path: data;\n",
              "\n",
              "Valid: LabelList (3000 items)\n",
              "x: TextList\n",
              "xxbos .. когда - то в далеком xxunk я купил две xxunk xxup городок и xxup поездка в xxup америку с xxup эдди xxup xxunk и хотя у меня у меня не было xxunk , я положил их на самом видном месте ... мои две главные духовные xxunk на тот довольно серьезный момент мой жизни ... ... и воспринимаю xxup xxunk и xxup xxunk не как xxunk , а как представителей духовного сопротивления ... ... таких как xxup xxunk , xxup бодров , xxup пелевин ...,xxbos xxmaj отличный образчик боевой фантастики . xxmaj особенно понравилась детальная проработка персонажей , а также продуманная модель боевых действий в космосе . а то умиляют авторы которые тупо берут скажем морскую xxunk и тупо xxunk в космос .,xxbos xxmaj xxunk . xxmaj юмор , владение сюжетной линией . а техника ! xxmaj xxunk рифмы , авторские xxunk ... xxmaj профессионально , легко - по - настоящему здорово .,xxbos xxmaj самая любимая книга у xxmaj коэльо . xxmaj паоло xxmaj коэльо как всегда - говорит просто , а думать заставляет . xxmaj обязательно прочитайте , оно того стоит .,xxbos xxmaj нормально . xxmaj но желания сразу перечитать не возникло . xxmaj отдельные моменты из книги xxunk .\n",
              "y: CategoryList\n",
              "10,9,9,9,7\n",
              "Path: data;\n",
              "\n",
              "Test: LabelList (1000 items)\n",
              "x: TextList\n",
              "xxbos ну xxunk там все понятно было xxunk очень xxunk красиво .,xxbos xxmaj мне книга понравилась - очень - этакая смесь мистики , приключений и любовного романа . xxmaj очень легко и интересно написана - лично мне оторваться было сложно .,xxbos в отношении книги меня можно записать в число фанатов . а фильм - попса .,xxbos нет определённого мнения насчёт этого фильма . вроде и не понравился , но и нет xxunk возражения против него . на один раз .,xxbos xxmaj тех кто считает , что « войны будут всегда , пока будут находиться люди готовые умирать » xxunk делает xxunk .\n",
              "y: EmptyLabelList\n",
              ",,,,\n",
              "Path: data, model=SequentialRNN(\n",
              "  (0): MultiBatchEncoder(\n",
              "    (module): AWD_LSTM(\n",
              "      (encoder): Embedding(41784, 400, padding_idx=1)\n",
              "      (encoder_dp): EmbeddingDropout(\n",
              "        (emb): Embedding(41784, 400, padding_idx=1)\n",
              "      )\n",
              "      (rnns): ModuleList(\n",
              "        (0): WeightDropout(\n",
              "          (module): LSTM(400, 1152, batch_first=True)\n",
              "        )\n",
              "        (1): WeightDropout(\n",
              "          (module): LSTM(1152, 1152, batch_first=True)\n",
              "        )\n",
              "        (2): WeightDropout(\n",
              "          (module): LSTM(1152, 400, batch_first=True)\n",
              "        )\n",
              "      )\n",
              "      (input_dp): RNNDropout()\n",
              "      (hidden_dps): ModuleList(\n",
              "        (0): RNNDropout()\n",
              "        (1): RNNDropout()\n",
              "        (2): RNNDropout()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (1): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=10, bias=True)\n",
              "    )\n",
              "  )\n",
              "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f7aa2a77f28>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('data'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
              "learn: RNNLearner(data=TextClasDataBunch;\n",
              "\n",
              "Train: LabelList (17000 items)\n",
              "x: TextList\n",
              "xxbos xxmaj лучшая вещь у xxmaj акунина . xxmaj явно выделяется из прочих , которые , по - моему , стали чересчур похожи друг на друга . xxmaj из новелл лучшая , кажется , об английском кладбище .,xxbos xxmaj отличная книга ! xxmaj хотя я и не увлекаюсь фентези , но тут что - то меня xxunk прочитать эту книгу по многочисленным рекомендациям клуба . xxmaj не разачарована . xxmaj как пишут многие , действительно было искренне жаль дочитывать . xxmaj обычно я xxunk прочесть поскорее , узнать развязку , но тут с самого начала было ясно что добро победит зло :) и я наслаждалась слогом , юмором , доброй сказкой :) xxmaj хорошо что книги есть продолжения :) xxmaj замечательно,xxbos книга не произвела xxunk,xxbos xxmaj не ожидала от xxmaj xxunk такой слабой вещи . xxmaj идея отличная - клинки и все такое , но читала с xxunk ... xxmaj хотя , может это перевод такой дурацкий .,xxbos xxmaj xxunk супер личность . xxmaj но , это далеко не лучшая книга о нем .\n",
              "y: CategoryList\n",
              "9,9,3,6,4\n",
              "Path: data;\n",
              "\n",
              "Valid: LabelList (3000 items)\n",
              "x: TextList\n",
              "xxbos .. когда - то в далеком xxunk я купил две xxunk xxup городок и xxup поездка в xxup америку с xxup эдди xxup xxunk и хотя у меня у меня не было xxunk , я положил их на самом видном месте ... мои две главные духовные xxunk на тот довольно серьезный момент мой жизни ... ... и воспринимаю xxup xxunk и xxup xxunk не как xxunk , а как представителей духовного сопротивления ... ... таких как xxup xxunk , xxup бодров , xxup пелевин ...,xxbos xxmaj отличный образчик боевой фантастики . xxmaj особенно понравилась детальная проработка персонажей , а также продуманная модель боевых действий в космосе . а то умиляют авторы которые тупо берут скажем морскую xxunk и тупо xxunk в космос .,xxbos xxmaj xxunk . xxmaj юмор , владение сюжетной линией . а техника ! xxmaj xxunk рифмы , авторские xxunk ... xxmaj профессионально , легко - по - настоящему здорово .,xxbos xxmaj самая любимая книга у xxmaj коэльо . xxmaj паоло xxmaj коэльо как всегда - говорит просто , а думать заставляет . xxmaj обязательно прочитайте , оно того стоит .,xxbos xxmaj нормально . xxmaj но желания сразу перечитать не возникло . xxmaj отдельные моменты из книги xxunk .\n",
              "y: CategoryList\n",
              "10,9,9,9,7\n",
              "Path: data;\n",
              "\n",
              "Test: LabelList (1000 items)\n",
              "x: TextList\n",
              "xxbos ну xxunk там все понятно было xxunk очень xxunk красиво .,xxbos xxmaj мне книга понравилась - очень - этакая смесь мистики , приключений и любовного романа . xxmaj очень легко и интересно написана - лично мне оторваться было сложно .,xxbos в отношении книги меня можно записать в число фанатов . а фильм - попса .,xxbos нет определённого мнения насчёт этого фильма . вроде и не понравился , но и нет xxunk возражения против него . на один раз .,xxbos xxmaj тех кто считает , что « войны будут всегда , пока будут находиться люди готовые умирать » xxunk делает xxunk .\n",
              "y: EmptyLabelList\n",
              ",,,,\n",
              "Path: data, model=SequentialRNN(\n",
              "  (0): MultiBatchEncoder(\n",
              "    (module): AWD_LSTM(\n",
              "      (encoder): Embedding(41784, 400, padding_idx=1)\n",
              "      (encoder_dp): EmbeddingDropout(\n",
              "        (emb): Embedding(41784, 400, padding_idx=1)\n",
              "      )\n",
              "      (rnns): ModuleList(\n",
              "        (0): WeightDropout(\n",
              "          (module): LSTM(400, 1152, batch_first=True)\n",
              "        )\n",
              "        (1): WeightDropout(\n",
              "          (module): LSTM(1152, 1152, batch_first=True)\n",
              "        )\n",
              "        (2): WeightDropout(\n",
              "          (module): LSTM(1152, 400, batch_first=True)\n",
              "        )\n",
              "      )\n",
              "      (input_dp): RNNDropout()\n",
              "      (hidden_dps): ModuleList(\n",
              "        (0): RNNDropout()\n",
              "        (1): RNNDropout()\n",
              "        (2): RNNDropout()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (1): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=10, bias=True)\n",
              "    )\n",
              "  )\n",
              "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f7aa2a77f28>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('data'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[...], layer_groups=[Sequential(\n",
              "  (0): Embedding(41784, 400, padding_idx=1)\n",
              "  (1): EmbeddingDropout(\n",
              "    (emb): Embedding(41784, 400, padding_idx=1)\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(400, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 400, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=10, bias=True)\n",
              "    )\n",
              "  )\n",
              ")], add_time=True, silent=False)\n",
              "alpha: 2.0\n",
              "beta: 1.0], layer_groups=[Sequential(\n",
              "  (0): Embedding(41784, 400, padding_idx=1)\n",
              "  (1): EmbeddingDropout(\n",
              "    (emb): Embedding(41784, 400, padding_idx=1)\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(400, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 400, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=10, bias=True)\n",
              "    )\n",
              "  )\n",
              ")], add_time=True, silent=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaHWrn46r9ds",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "1388cbc9-4c58-4f2c-ed86-11867180e927"
      },
      "source": [
        "learn.freeze_to(-3)\n",
        "# Slice up the learning rates to apply different rates to different layers of the network - the deeper the larger.\n",
        "learn.fit_one_cycle(1, slice(5e-3/2., 5e-3))"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.716906</td>\n",
              "      <td>1.628634</td>\n",
              "      <td>0.361000</td>\n",
              "      <td>04:22</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSV45Q33tRDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.save('stage-3-model.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SR4dPkqJtUES",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "85d6a4d2-955d-4df7-e0e6-ef8d64d859fe"
      },
      "source": [
        "learn.load('stage-3-model.pkl')"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNLearner(data=TextClasDataBunch;\n",
              "\n",
              "Train: LabelList (17000 items)\n",
              "x: TextList\n",
              "xxbos xxmaj лучшая вещь у xxmaj акунина . xxmaj явно выделяется из прочих , которые , по - моему , стали чересчур похожи друг на друга . xxmaj из новелл лучшая , кажется , об английском кладбище .,xxbos xxmaj отличная книга ! xxmaj хотя я и не увлекаюсь фентези , но тут что - то меня xxunk прочитать эту книгу по многочисленным рекомендациям клуба . xxmaj не разачарована . xxmaj как пишут многие , действительно было искренне жаль дочитывать . xxmaj обычно я xxunk прочесть поскорее , узнать развязку , но тут с самого начала было ясно что добро победит зло :) и я наслаждалась слогом , юмором , доброй сказкой :) xxmaj хорошо что книги есть продолжения :) xxmaj замечательно,xxbos книга не произвела xxunk,xxbos xxmaj не ожидала от xxmaj xxunk такой слабой вещи . xxmaj идея отличная - клинки и все такое , но читала с xxunk ... xxmaj хотя , может это перевод такой дурацкий .,xxbos xxmaj xxunk супер личность . xxmaj но , это далеко не лучшая книга о нем .\n",
              "y: CategoryList\n",
              "9,9,3,6,4\n",
              "Path: data;\n",
              "\n",
              "Valid: LabelList (3000 items)\n",
              "x: TextList\n",
              "xxbos .. когда - то в далеком xxunk я купил две xxunk xxup городок и xxup поездка в xxup америку с xxup эдди xxup xxunk и хотя у меня у меня не было xxunk , я положил их на самом видном месте ... мои две главные духовные xxunk на тот довольно серьезный момент мой жизни ... ... и воспринимаю xxup xxunk и xxup xxunk не как xxunk , а как представителей духовного сопротивления ... ... таких как xxup xxunk , xxup бодров , xxup пелевин ...,xxbos xxmaj отличный образчик боевой фантастики . xxmaj особенно понравилась детальная проработка персонажей , а также продуманная модель боевых действий в космосе . а то умиляют авторы которые тупо берут скажем морскую xxunk и тупо xxunk в космос .,xxbos xxmaj xxunk . xxmaj юмор , владение сюжетной линией . а техника ! xxmaj xxunk рифмы , авторские xxunk ... xxmaj профессионально , легко - по - настоящему здорово .,xxbos xxmaj самая любимая книга у xxmaj коэльо . xxmaj паоло xxmaj коэльо как всегда - говорит просто , а думать заставляет . xxmaj обязательно прочитайте , оно того стоит .,xxbos xxmaj нормально . xxmaj но желания сразу перечитать не возникло . xxmaj отдельные моменты из книги xxunk .\n",
              "y: CategoryList\n",
              "10,9,9,9,7\n",
              "Path: data;\n",
              "\n",
              "Test: LabelList (1000 items)\n",
              "x: TextList\n",
              "xxbos ну xxunk там все понятно было xxunk очень xxunk красиво .,xxbos xxmaj мне книга понравилась - очень - этакая смесь мистики , приключений и любовного романа . xxmaj очень легко и интересно написана - лично мне оторваться было сложно .,xxbos в отношении книги меня можно записать в число фанатов . а фильм - попса .,xxbos нет определённого мнения насчёт этого фильма . вроде и не понравился , но и нет xxunk возражения против него . на один раз .,xxbos xxmaj тех кто считает , что « войны будут всегда , пока будут находиться люди готовые умирать » xxunk делает xxunk .\n",
              "y: EmptyLabelList\n",
              ",,,,\n",
              "Path: data, model=SequentialRNN(\n",
              "  (0): MultiBatchEncoder(\n",
              "    (module): AWD_LSTM(\n",
              "      (encoder): Embedding(41784, 400, padding_idx=1)\n",
              "      (encoder_dp): EmbeddingDropout(\n",
              "        (emb): Embedding(41784, 400, padding_idx=1)\n",
              "      )\n",
              "      (rnns): ModuleList(\n",
              "        (0): WeightDropout(\n",
              "          (module): LSTM(400, 1152, batch_first=True)\n",
              "        )\n",
              "        (1): WeightDropout(\n",
              "          (module): LSTM(1152, 1152, batch_first=True)\n",
              "        )\n",
              "        (2): WeightDropout(\n",
              "          (module): LSTM(1152, 400, batch_first=True)\n",
              "        )\n",
              "      )\n",
              "      (input_dp): RNNDropout()\n",
              "      (hidden_dps): ModuleList(\n",
              "        (0): RNNDropout()\n",
              "        (1): RNNDropout()\n",
              "        (2): RNNDropout()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (1): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=10, bias=True)\n",
              "    )\n",
              "  )\n",
              "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f7aa2a77f28>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('data'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
              "learn: RNNLearner(data=TextClasDataBunch;\n",
              "\n",
              "Train: LabelList (17000 items)\n",
              "x: TextList\n",
              "xxbos xxmaj лучшая вещь у xxmaj акунина . xxmaj явно выделяется из прочих , которые , по - моему , стали чересчур похожи друг на друга . xxmaj из новелл лучшая , кажется , об английском кладбище .,xxbos xxmaj отличная книга ! xxmaj хотя я и не увлекаюсь фентези , но тут что - то меня xxunk прочитать эту книгу по многочисленным рекомендациям клуба . xxmaj не разачарована . xxmaj как пишут многие , действительно было искренне жаль дочитывать . xxmaj обычно я xxunk прочесть поскорее , узнать развязку , но тут с самого начала было ясно что добро победит зло :) и я наслаждалась слогом , юмором , доброй сказкой :) xxmaj хорошо что книги есть продолжения :) xxmaj замечательно,xxbos книга не произвела xxunk,xxbos xxmaj не ожидала от xxmaj xxunk такой слабой вещи . xxmaj идея отличная - клинки и все такое , но читала с xxunk ... xxmaj хотя , может это перевод такой дурацкий .,xxbos xxmaj xxunk супер личность . xxmaj но , это далеко не лучшая книга о нем .\n",
              "y: CategoryList\n",
              "9,9,3,6,4\n",
              "Path: data;\n",
              "\n",
              "Valid: LabelList (3000 items)\n",
              "x: TextList\n",
              "xxbos .. когда - то в далеком xxunk я купил две xxunk xxup городок и xxup поездка в xxup америку с xxup эдди xxup xxunk и хотя у меня у меня не было xxunk , я положил их на самом видном месте ... мои две главные духовные xxunk на тот довольно серьезный момент мой жизни ... ... и воспринимаю xxup xxunk и xxup xxunk не как xxunk , а как представителей духовного сопротивления ... ... таких как xxup xxunk , xxup бодров , xxup пелевин ...,xxbos xxmaj отличный образчик боевой фантастики . xxmaj особенно понравилась детальная проработка персонажей , а также продуманная модель боевых действий в космосе . а то умиляют авторы которые тупо берут скажем морскую xxunk и тупо xxunk в космос .,xxbos xxmaj xxunk . xxmaj юмор , владение сюжетной линией . а техника ! xxmaj xxunk рифмы , авторские xxunk ... xxmaj профессионально , легко - по - настоящему здорово .,xxbos xxmaj самая любимая книга у xxmaj коэльо . xxmaj паоло xxmaj коэльо как всегда - говорит просто , а думать заставляет . xxmaj обязательно прочитайте , оно того стоит .,xxbos xxmaj нормально . xxmaj но желания сразу перечитать не возникло . xxmaj отдельные моменты из книги xxunk .\n",
              "y: CategoryList\n",
              "10,9,9,9,7\n",
              "Path: data;\n",
              "\n",
              "Test: LabelList (1000 items)\n",
              "x: TextList\n",
              "xxbos ну xxunk там все понятно было xxunk очень xxunk красиво .,xxbos xxmaj мне книга понравилась - очень - этакая смесь мистики , приключений и любовного романа . xxmaj очень легко и интересно написана - лично мне оторваться было сложно .,xxbos в отношении книги меня можно записать в число фанатов . а фильм - попса .,xxbos нет определённого мнения насчёт этого фильма . вроде и не понравился , но и нет xxunk возражения против него . на один раз .,xxbos xxmaj тех кто считает , что « войны будут всегда , пока будут находиться люди готовые умирать » xxunk делает xxunk .\n",
              "y: EmptyLabelList\n",
              ",,,,\n",
              "Path: data, model=SequentialRNN(\n",
              "  (0): MultiBatchEncoder(\n",
              "    (module): AWD_LSTM(\n",
              "      (encoder): Embedding(41784, 400, padding_idx=1)\n",
              "      (encoder_dp): EmbeddingDropout(\n",
              "        (emb): Embedding(41784, 400, padding_idx=1)\n",
              "      )\n",
              "      (rnns): ModuleList(\n",
              "        (0): WeightDropout(\n",
              "          (module): LSTM(400, 1152, batch_first=True)\n",
              "        )\n",
              "        (1): WeightDropout(\n",
              "          (module): LSTM(1152, 1152, batch_first=True)\n",
              "        )\n",
              "        (2): WeightDropout(\n",
              "          (module): LSTM(1152, 400, batch_first=True)\n",
              "        )\n",
              "      )\n",
              "      (input_dp): RNNDropout()\n",
              "      (hidden_dps): ModuleList(\n",
              "        (0): RNNDropout()\n",
              "        (1): RNNDropout()\n",
              "        (2): RNNDropout()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (1): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=10, bias=True)\n",
              "    )\n",
              "  )\n",
              "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f7aa2a77f28>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('data'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[...], layer_groups=[Sequential(\n",
              "  (0): Embedding(41784, 400, padding_idx=1)\n",
              "  (1): EmbeddingDropout(\n",
              "    (emb): Embedding(41784, 400, padding_idx=1)\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(400, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 400, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=10, bias=True)\n",
              "    )\n",
              "  )\n",
              ")], add_time=True, silent=False)\n",
              "alpha: 2.0\n",
              "beta: 1.0], layer_groups=[Sequential(\n",
              "  (0): Embedding(41784, 400, padding_idx=1)\n",
              "  (1): EmbeddingDropout(\n",
              "    (emb): Embedding(41784, 400, padding_idx=1)\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(400, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 400, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=10, bias=True)\n",
              "    )\n",
              "  )\n",
              ")], add_time=True, silent=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6n2HyqkpXGn5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "91d9c0b6-674d-4ac1-f388-44b72ea4ff5e"
      },
      "source": [
        "learn.predict(\"Плохо, но отвращения не вызывает, что уже хорошо. \\\n",
        "Если по сути, то сценарий - полный бред. Тут уже писали, что странно, \\\n",
        "что лилипуты, которые находятся еще в рыцарском веке, вдруг раз и построили \\\n",
        "современный дом для Гулливера, не говоря уже об электричестве.Ну ладно, может они такие просвещенные, \\\n",
        "что предпочитают беречь природу, а не засорять ее техногенными отходами. Но когда главный злодей умудрился сделать \\\n",
        "робота по инструкции из какого-то подросткового-нердовского журнала \\\"Как сделать робота\\\" на один разворот, это уже \\\n",
        "никак не объяснить вообще, кроме как пофигизма создателей фильма. Плюс к тому остается загадкой, как Гулливер и Дарси \\\n",
        "выбрались с острова. Туда они попали непонятно как тоже, ну если допустить, что смерч из бермудского треугольника выносит \\\n",
        "всех в него попавших на лилипутский остров, то можно жить. А куда он выкидывает людей, если они идут в обратном \\\n",
        "направлении? Прямо на остров Манхеттен? Что касается главного героя - то абсолютно несимпатичнй мне персонаж. \\\n",
        "Опять параллель с Иванушкой -Дурачком. Ничего особого не делал, никакими талантами не обладал, тут по \\\n",
        "счастливой случайности вляпался в историю, счастливым образом вышел из нее сухим и получил полкоралевства и царевну впридачу. \\\n",
        "А его товарищ, который работает и пытается совим горбом получить хоть какое-то повышение остается ни с чем. \\\n",
        "вообще не очень люблю такие сказки, потому что они запечатляют в детских головах четкий импринт, что \\\n",
        "много работать - плохо, а мало работать - хорошо. А отношения главных героев вообще вставили, наверное, \\\n",
        "потому что \\\"как же без них\\\", но абсолютно без души. Даже в подростковых комедиях типа \\\"Американского пирога\\\" \\\n",
        "пары подбирают куда лучше. Здесь же, при поцелуе не вздрагивает ни одна романтическая жилка. \\\n",
        "Добавили бы хоть какую-нибудь искорку.\")"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Category 8,\n",
              " tensor(9),\n",
              " tensor([1.7086e-05, 1.6910e-02, 1.3306e-01, 1.7865e-02, 4.1020e-02, 8.0780e-02,\n",
              "         1.2592e-01, 9.8397e-02, 1.3294e-01, 1.8242e-01, 1.7068e-01]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnjlIuCZZF-B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "d5974a75-2eba-4058-d077-bdaefdc98126"
      },
      "source": [
        "learn.predict(\"Я не считаю что это лучшая книга -- скорее я бы так охарактеризовал одну из первых книг серии, но написано замечательно. \\\n",
        "Плюс отсылка к Куприну забавная. Плюс наконец то объясняет предысторию событий.\")"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Category 10,\n",
              " tensor(2),\n",
              " tensor([2.2118e-05, 3.6257e-02, 2.7368e-01, 2.9112e-02, 3.7280e-02, 5.0455e-02,\n",
              "         7.0328e-02, 5.2051e-02, 8.1394e-02, 1.4846e-01, 2.2096e-01]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9T1DEnrzZLQJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "3698697f-05ec-4888-e73f-f4de4e5aff9a"
      },
      "source": [
        "learn.predict(\"от первого фильма было странное чувство: когда на экране нет Михалкова, \\\n",
        "с трудом но смотришь.как только появляется - полный капец. посмотрев, по рекомендации Задорнова, \\\n",
        "второй фильм, понял в чем дело.образ комдива в этих продолжениях насквозь сжив. попытка скрестить Котовского с \\\n",
        "Рокоссовским отвратительна.такие как Котов,умеющие только рубать шашкой монахов, топить баржами пленных беляков да \\\n",
        "травить газами несчастных русских мужиков, сами в атаку сроду не \\\n",
        "ходили и талантами стратегов не обладали.и правильно Сталин таких Котовых порасстрелял перед войной.\\\n",
        "почитайте Суворова\\\"Очищение\\\"\")"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Category 9,\n",
              " tensor(10),\n",
              " tensor([7.6083e-06, 2.2518e-02, 2.2999e-01, 1.7698e-02, 2.9450e-02, 4.6191e-02,\n",
              "         6.1172e-02, 6.6724e-02, 1.0308e-01, 1.7741e-01, 2.4576e-01]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNJgRrqEaXt5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "fa3d1c5c-bd15-4eec-fb79-30bd66a4a58b"
      },
      "source": [
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(1, slice(2e-3/100, 2e-3))"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.663359</td>\n",
              "      <td>1.590418</td>\n",
              "      <td>0.374000</td>\n",
              "      <td>07:12</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kvi1hSJ0b-6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.save('final-model.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8Ln2p8Kb_f2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d24b48a6-3004-4a6f-a841-a5b89399561e"
      },
      "source": [
        "learn.load('final-model.pkl')"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNLearner(data=TextClasDataBunch;\n",
              "\n",
              "Train: LabelList (17000 items)\n",
              "x: TextList\n",
              "xxbos xxmaj лучшая вещь у xxmaj акунина . xxmaj явно выделяется из прочих , которые , по - моему , стали чересчур похожи друг на друга . xxmaj из новелл лучшая , кажется , об английском кладбище .,xxbos xxmaj отличная книга ! xxmaj хотя я и не увлекаюсь фентези , но тут что - то меня xxunk прочитать эту книгу по многочисленным рекомендациям клуба . xxmaj не разачарована . xxmaj как пишут многие , действительно было искренне жаль дочитывать . xxmaj обычно я xxunk прочесть поскорее , узнать развязку , но тут с самого начала было ясно что добро победит зло :) и я наслаждалась слогом , юмором , доброй сказкой :) xxmaj хорошо что книги есть продолжения :) xxmaj замечательно,xxbos книга не произвела xxunk,xxbos xxmaj не ожидала от xxmaj xxunk такой слабой вещи . xxmaj идея отличная - клинки и все такое , но читала с xxunk ... xxmaj хотя , может это перевод такой дурацкий .,xxbos xxmaj xxunk супер личность . xxmaj но , это далеко не лучшая книга о нем .\n",
              "y: CategoryList\n",
              "9,9,3,6,4\n",
              "Path: data;\n",
              "\n",
              "Valid: LabelList (3000 items)\n",
              "x: TextList\n",
              "xxbos .. когда - то в далеком xxunk я купил две xxunk xxup городок и xxup поездка в xxup америку с xxup эдди xxup xxunk и хотя у меня у меня не было xxunk , я положил их на самом видном месте ... мои две главные духовные xxunk на тот довольно серьезный момент мой жизни ... ... и воспринимаю xxup xxunk и xxup xxunk не как xxunk , а как представителей духовного сопротивления ... ... таких как xxup xxunk , xxup бодров , xxup пелевин ...,xxbos xxmaj отличный образчик боевой фантастики . xxmaj особенно понравилась детальная проработка персонажей , а также продуманная модель боевых действий в космосе . а то умиляют авторы которые тупо берут скажем морскую xxunk и тупо xxunk в космос .,xxbos xxmaj xxunk . xxmaj юмор , владение сюжетной линией . а техника ! xxmaj xxunk рифмы , авторские xxunk ... xxmaj профессионально , легко - по - настоящему здорово .,xxbos xxmaj самая любимая книга у xxmaj коэльо . xxmaj паоло xxmaj коэльо как всегда - говорит просто , а думать заставляет . xxmaj обязательно прочитайте , оно того стоит .,xxbos xxmaj нормально . xxmaj но желания сразу перечитать не возникло . xxmaj отдельные моменты из книги xxunk .\n",
              "y: CategoryList\n",
              "10,9,9,9,7\n",
              "Path: data;\n",
              "\n",
              "Test: LabelList (1000 items)\n",
              "x: TextList\n",
              "xxbos ну xxunk там все понятно было xxunk очень xxunk красиво .,xxbos xxmaj мне книга понравилась - очень - этакая смесь мистики , приключений и любовного романа . xxmaj очень легко и интересно написана - лично мне оторваться было сложно .,xxbos в отношении книги меня можно записать в число фанатов . а фильм - попса .,xxbos нет определённого мнения насчёт этого фильма . вроде и не понравился , но и нет xxunk возражения против него . на один раз .,xxbos xxmaj тех кто считает , что « войны будут всегда , пока будут находиться люди готовые умирать » xxunk делает xxunk .\n",
              "y: EmptyLabelList\n",
              ",,,,\n",
              "Path: data, model=SequentialRNN(\n",
              "  (0): MultiBatchEncoder(\n",
              "    (module): AWD_LSTM(\n",
              "      (encoder): Embedding(41784, 400, padding_idx=1)\n",
              "      (encoder_dp): EmbeddingDropout(\n",
              "        (emb): Embedding(41784, 400, padding_idx=1)\n",
              "      )\n",
              "      (rnns): ModuleList(\n",
              "        (0): WeightDropout(\n",
              "          (module): LSTM(400, 1152, batch_first=True)\n",
              "        )\n",
              "        (1): WeightDropout(\n",
              "          (module): LSTM(1152, 1152, batch_first=True)\n",
              "        )\n",
              "        (2): WeightDropout(\n",
              "          (module): LSTM(1152, 400, batch_first=True)\n",
              "        )\n",
              "      )\n",
              "      (input_dp): RNNDropout()\n",
              "      (hidden_dps): ModuleList(\n",
              "        (0): RNNDropout()\n",
              "        (1): RNNDropout()\n",
              "        (2): RNNDropout()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (1): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=10, bias=True)\n",
              "    )\n",
              "  )\n",
              "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f7aa2a77f28>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('data'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
              "learn: RNNLearner(data=TextClasDataBunch;\n",
              "\n",
              "Train: LabelList (17000 items)\n",
              "x: TextList\n",
              "xxbos xxmaj лучшая вещь у xxmaj акунина . xxmaj явно выделяется из прочих , которые , по - моему , стали чересчур похожи друг на друга . xxmaj из новелл лучшая , кажется , об английском кладбище .,xxbos xxmaj отличная книга ! xxmaj хотя я и не увлекаюсь фентези , но тут что - то меня xxunk прочитать эту книгу по многочисленным рекомендациям клуба . xxmaj не разачарована . xxmaj как пишут многие , действительно было искренне жаль дочитывать . xxmaj обычно я xxunk прочесть поскорее , узнать развязку , но тут с самого начала было ясно что добро победит зло :) и я наслаждалась слогом , юмором , доброй сказкой :) xxmaj хорошо что книги есть продолжения :) xxmaj замечательно,xxbos книга не произвела xxunk,xxbos xxmaj не ожидала от xxmaj xxunk такой слабой вещи . xxmaj идея отличная - клинки и все такое , но читала с xxunk ... xxmaj хотя , может это перевод такой дурацкий .,xxbos xxmaj xxunk супер личность . xxmaj но , это далеко не лучшая книга о нем .\n",
              "y: CategoryList\n",
              "9,9,3,6,4\n",
              "Path: data;\n",
              "\n",
              "Valid: LabelList (3000 items)\n",
              "x: TextList\n",
              "xxbos .. когда - то в далеком xxunk я купил две xxunk xxup городок и xxup поездка в xxup америку с xxup эдди xxup xxunk и хотя у меня у меня не было xxunk , я положил их на самом видном месте ... мои две главные духовные xxunk на тот довольно серьезный момент мой жизни ... ... и воспринимаю xxup xxunk и xxup xxunk не как xxunk , а как представителей духовного сопротивления ... ... таких как xxup xxunk , xxup бодров , xxup пелевин ...,xxbos xxmaj отличный образчик боевой фантастики . xxmaj особенно понравилась детальная проработка персонажей , а также продуманная модель боевых действий в космосе . а то умиляют авторы которые тупо берут скажем морскую xxunk и тупо xxunk в космос .,xxbos xxmaj xxunk . xxmaj юмор , владение сюжетной линией . а техника ! xxmaj xxunk рифмы , авторские xxunk ... xxmaj профессионально , легко - по - настоящему здорово .,xxbos xxmaj самая любимая книга у xxmaj коэльо . xxmaj паоло xxmaj коэльо как всегда - говорит просто , а думать заставляет . xxmaj обязательно прочитайте , оно того стоит .,xxbos xxmaj нормально . xxmaj но желания сразу перечитать не возникло . xxmaj отдельные моменты из книги xxunk .\n",
              "y: CategoryList\n",
              "10,9,9,9,7\n",
              "Path: data;\n",
              "\n",
              "Test: LabelList (1000 items)\n",
              "x: TextList\n",
              "xxbos ну xxunk там все понятно было xxunk очень xxunk красиво .,xxbos xxmaj мне книга понравилась - очень - этакая смесь мистики , приключений и любовного романа . xxmaj очень легко и интересно написана - лично мне оторваться было сложно .,xxbos в отношении книги меня можно записать в число фанатов . а фильм - попса .,xxbos нет определённого мнения насчёт этого фильма . вроде и не понравился , но и нет xxunk возражения против него . на один раз .,xxbos xxmaj тех кто считает , что « войны будут всегда , пока будут находиться люди готовые умирать » xxunk делает xxunk .\n",
              "y: EmptyLabelList\n",
              ",,,,\n",
              "Path: data, model=SequentialRNN(\n",
              "  (0): MultiBatchEncoder(\n",
              "    (module): AWD_LSTM(\n",
              "      (encoder): Embedding(41784, 400, padding_idx=1)\n",
              "      (encoder_dp): EmbeddingDropout(\n",
              "        (emb): Embedding(41784, 400, padding_idx=1)\n",
              "      )\n",
              "      (rnns): ModuleList(\n",
              "        (0): WeightDropout(\n",
              "          (module): LSTM(400, 1152, batch_first=True)\n",
              "        )\n",
              "        (1): WeightDropout(\n",
              "          (module): LSTM(1152, 1152, batch_first=True)\n",
              "        )\n",
              "        (2): WeightDropout(\n",
              "          (module): LSTM(1152, 400, batch_first=True)\n",
              "        )\n",
              "      )\n",
              "      (input_dp): RNNDropout()\n",
              "      (hidden_dps): ModuleList(\n",
              "        (0): RNNDropout()\n",
              "        (1): RNNDropout()\n",
              "        (2): RNNDropout()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (1): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=10, bias=True)\n",
              "    )\n",
              "  )\n",
              "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f7aa2a77f28>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('data'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[...], layer_groups=[Sequential(\n",
              "  (0): Embedding(41784, 400, padding_idx=1)\n",
              "  (1): EmbeddingDropout(\n",
              "    (emb): Embedding(41784, 400, padding_idx=1)\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(400, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 400, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=10, bias=True)\n",
              "    )\n",
              "  )\n",
              ")], add_time=True, silent=False)\n",
              "alpha: 2.0\n",
              "beta: 1.0], layer_groups=[Sequential(\n",
              "  (0): Embedding(41784, 400, padding_idx=1)\n",
              "  (1): EmbeddingDropout(\n",
              "    (emb): Embedding(41784, 400, padding_idx=1)\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(400, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 400, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=10, bias=True)\n",
              "    )\n",
              "  )\n",
              ")], add_time=True, silent=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY0-HpqgiGQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.predict(\"Сериал очень люблю, но Академия и Земля вызывает у меня отторжение идеей... \\\n",
        "Не люблю, когда принижают ценность человека как личности, даже не смотря на ошибки личности...\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvPBDjwob_wQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "10b699db-1fb2-4498-b759-a619f0fde7e5"
      },
      "source": [
        "for text in texts_test:\n",
        "  print(learn.predict(text)[0])"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9\n",
            "9\n",
            "10\n",
            "5\n",
            "10\n",
            "9\n",
            "10\n",
            "10\n",
            "10\n",
            "9\n",
            "10\n",
            "7\n",
            "9\n",
            "5\n",
            "8\n",
            "10\n",
            "9\n",
            "10\n",
            "10\n",
            "10\n",
            "2\n",
            "10\n",
            "9\n",
            "9\n",
            "10\n",
            "3\n",
            "9\n",
            "8\n",
            "5\n",
            "9\n",
            "10\n",
            "9\n",
            "10\n",
            "9\n",
            "10\n",
            "10\n",
            "7\n",
            "9\n",
            "9\n",
            "10\n",
            "10\n",
            "9\n",
            "7\n",
            "8\n",
            "10\n",
            "8\n",
            "7\n",
            "10\n",
            "10\n",
            "10\n",
            "10\n",
            "10\n",
            "8\n",
            "8\n",
            "5\n",
            "9\n",
            "10\n",
            "10\n",
            "9\n",
            "10\n",
            "9\n",
            "9\n",
            "9\n",
            "9\n",
            "10\n",
            "8\n",
            "10\n",
            "9\n",
            "10\n",
            "10\n",
            "7\n",
            "8\n",
            "8\n",
            "10\n",
            "8\n",
            "9\n",
            "7\n",
            "10\n",
            "8\n",
            "8\n",
            "8\n",
            "10\n",
            "8\n",
            "10\n",
            "8\n",
            "7\n",
            "8\n",
            "9\n",
            "9\n",
            "8\n",
            "10\n",
            "5\n",
            "9\n",
            "9\n",
            "10\n",
            "8\n",
            "7\n",
            "9\n",
            "10\n",
            "10\n",
            "10\n",
            "9\n",
            "9\n",
            "8\n",
            "8\n",
            "8\n",
            "8\n",
            "10\n",
            "9\n",
            "10\n",
            "10\n",
            "8\n",
            "8\n",
            "10\n",
            "10\n",
            "9\n",
            "8\n",
            "10\n",
            "10\n",
            "9\n",
            "7\n",
            "8\n",
            "10\n",
            "9\n",
            "10\n",
            "9\n",
            "10\n",
            "10\n",
            "9\n",
            "10\n",
            "9\n",
            "10\n",
            "8\n",
            "8\n",
            "8\n",
            "10\n",
            "5\n",
            "9\n",
            "10\n",
            "10\n",
            "8\n",
            "7\n",
            "10\n",
            "9\n",
            "9\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "10\n",
            "9\n",
            "10\n",
            "9\n",
            "10\n",
            "10\n",
            "10\n",
            "5\n",
            "8\n",
            "9\n",
            "10\n",
            "10\n",
            "7\n",
            "10\n",
            "10\n",
            "9\n",
            "8\n",
            "9\n",
            "10\n",
            "8\n",
            "6\n",
            "7\n",
            "8\n",
            "5\n",
            "10\n",
            "5\n",
            "10\n",
            "10\n",
            "10\n",
            "10\n",
            "10\n",
            "8\n",
            "9\n",
            "9\n",
            "7\n",
            "10\n",
            "5\n",
            "8\n",
            "9\n",
            "10\n",
            "8\n",
            "8\n",
            "9\n",
            "5\n",
            "10\n",
            "9\n",
            "5\n",
            "10\n",
            "9\n",
            "9\n",
            "9\n",
            "7\n",
            "8\n",
            "10\n",
            "10\n",
            "9\n",
            "6\n",
            "8\n",
            "10\n",
            "9\n",
            "8\n",
            "9\n",
            "10\n",
            "10\n",
            "10\n",
            "7\n",
            "9\n",
            "8\n",
            "4\n",
            "9\n",
            "10\n",
            "10\n",
            "10\n",
            "8\n",
            "8\n",
            "10\n",
            "9\n",
            "10\n",
            "9\n",
            "10\n",
            "10\n",
            "10\n",
            "10\n",
            "10\n",
            "8\n",
            "9\n",
            "9\n",
            "8\n",
            "9\n",
            "9\n",
            "9\n",
            "9\n",
            "10\n",
            "10\n",
            "10\n",
            "10\n",
            "7\n",
            "10\n",
            "9\n",
            "5\n",
            "10\n",
            "8\n",
            "8\n",
            "9\n",
            "9\n",
            "10\n",
            "10\n",
            "10\n",
            "7\n",
            "5\n",
            "9\n",
            "9\n",
            "9\n",
            "9\n",
            "8\n",
            "10\n",
            "8\n",
            "10\n",
            "8\n",
            "9\n",
            "8\n",
            "10\n",
            "8\n",
            "9\n",
            "9\n",
            "10\n",
            "9\n",
            "10\n",
            "9\n",
            "8\n",
            "8\n",
            "5\n",
            "8\n",
            "9\n",
            "10\n",
            "10\n",
            "7\n",
            "10\n",
            "10\n",
            "9\n",
            "9\n",
            "10\n",
            "9\n",
            "10\n",
            "9\n",
            "7\n",
            "10\n",
            "5\n",
            "8\n",
            "9\n",
            "9\n",
            "9\n",
            "9\n",
            "9\n",
            "5\n",
            "8\n",
            "9\n",
            "10\n",
            "10\n",
            "10\n",
            "10\n",
            "7\n",
            "8\n",
            "8\n",
            "10\n",
            "10\n",
            "8\n",
            "10\n",
            "10\n",
            "9\n",
            "8\n",
            "9\n",
            "10\n",
            "10\n",
            "9\n",
            "10\n",
            "10\n",
            "10\n",
            "9\n",
            "10\n",
            "9\n",
            "7\n",
            "4\n",
            "9\n",
            "9\n",
            "8\n",
            "10\n",
            "7\n",
            "10\n",
            "5\n",
            "8\n",
            "10\n",
            "10\n",
            "10\n",
            "10\n",
            "8\n",
            "8\n",
            "9\n",
            "9\n",
            "8\n",
            "9\n",
            "10\n",
            "9\n",
            "10\n",
            "9\n",
            "9\n",
            "5\n",
            "10\n",
            "7\n",
            "10\n",
            "9\n",
            "9\n",
            "10\n",
            "10\n",
            "10\n",
            "10\n",
            "8\n",
            "10\n",
            "7\n",
            "10\n",
            "10\n",
            "8\n",
            "10\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "10\n",
            "9\n",
            "8\n",
            "9\n",
            "10\n",
            "9\n",
            "10\n",
            "9\n",
            "10\n",
            "8\n",
            "9\n",
            "10\n",
            "7\n",
            "8\n",
            "10\n",
            "9\n",
            "10\n",
            "7\n",
            "10\n",
            "8\n",
            "10\n",
            "10\n",
            "9\n",
            "9\n",
            "10\n",
            "9\n",
            "8\n",
            "8\n",
            "10\n",
            "8\n",
            "9\n",
            "9\n",
            "10\n",
            "9\n",
            "9\n",
            "8\n",
            "10\n",
            "10\n",
            "9\n",
            "8\n",
            "9\n",
            "10\n",
            "7\n",
            "10\n",
            "8\n",
            "8\n",
            "8\n",
            "5\n",
            "10\n",
            "9\n",
            "10\n",
            "10\n",
            "10\n",
            "7\n",
            "9\n",
            "10\n",
            "7\n",
            "9\n",
            "8\n",
            "9\n",
            "9\n",
            "10\n",
            "10\n",
            "8\n",
            "8\n",
            "10\n",
            "9\n",
            "8\n",
            "8\n",
            "5\n",
            "10\n",
            "8\n",
            "9\n",
            "8\n",
            "10\n",
            "5\n",
            "9\n",
            "10\n",
            "8\n",
            "7\n",
            "9\n",
            "10\n",
            "8\n",
            "4\n",
            "2\n",
            "10\n",
            "10\n",
            "5\n",
            "10\n",
            "9\n",
            "8\n",
            "10\n",
            "10\n",
            "8\n",
            "7\n",
            "9\n",
            "9\n",
            "7\n",
            "10\n",
            "10\n",
            "7\n",
            "10\n",
            "10\n",
            "9\n",
            "10\n",
            "9\n",
            "10\n",
            "10\n",
            "8\n",
            "5\n",
            "10\n",
            "10\n",
            "10\n",
            "10\n",
            "10\n",
            "10\n",
            "5\n",
            "7\n",
            "9\n",
            "7\n",
            "8\n",
            "10\n",
            "8\n",
            "9\n",
            "9\n",
            "7\n",
            "9\n",
            "10\n",
            "3\n",
            "10\n",
            "8\n",
            "10\n",
            "9\n",
            "8\n",
            "8\n",
            "9\n",
            "7\n",
            "10\n",
            "10\n",
            "10\n",
            "8\n",
            "10\n",
            "9\n",
            "5\n",
            "5\n",
            "2\n",
            "10\n",
            "9\n",
            "7\n",
            "10\n",
            "7\n",
            "10\n",
            "8\n",
            "7\n",
            "9\n",
            "10\n",
            "8\n",
            "5\n",
            "8\n",
            "10\n",
            "7\n",
            "8\n",
            "8\n",
            "10\n",
            "10\n",
            "9\n",
            "10\n",
            "9\n",
            "5\n",
            "9\n",
            "9\n",
            "9\n",
            "8\n",
            "10\n",
            "8\n",
            "10\n",
            "9\n",
            "8\n",
            "9\n",
            "8\n",
            "9\n",
            "10\n",
            "9\n",
            "9\n",
            "10\n",
            "9\n",
            "9\n",
            "10\n",
            "10\n",
            "9\n",
            "7\n",
            "5\n",
            "7\n",
            "9\n",
            "8\n",
            "8\n",
            "10\n",
            "10\n",
            "8\n",
            "10\n",
            "9\n",
            "10\n",
            "10\n",
            "9\n",
            "9\n",
            "9\n",
            "10\n",
            "10\n",
            "9\n",
            "10\n",
            "10\n",
            "10\n",
            "10\n",
            "5\n",
            "10\n",
            "10\n",
            "7\n",
            "10\n",
            "8\n",
            "10\n",
            "10\n",
            "10\n",
            "7\n",
            "10\n",
            "8\n",
            "9\n",
            "9\n",
            "10\n",
            "10\n",
            "9\n",
            "8\n",
            "10\n",
            "8\n",
            "5\n",
            "10\n",
            "9\n",
            "9\n",
            "8\n",
            "10\n",
            "8\n",
            "9\n",
            "5\n",
            "8\n",
            "8\n",
            "8\n",
            "10\n",
            "8\n",
            "10\n",
            "7\n",
            "9\n",
            "10\n",
            "10\n",
            "9\n",
            "10\n",
            "8\n",
            "5\n",
            "4\n",
            "9\n",
            "8\n",
            "10\n",
            "10\n",
            "8\n",
            "9\n",
            "4\n",
            "9\n",
            "9\n",
            "9\n",
            "5\n",
            "9\n",
            "9\n",
            "1\n",
            "9\n",
            "10\n",
            "8\n",
            "9\n",
            "7\n",
            "10\n",
            "3\n",
            "10\n",
            "9\n",
            "3\n",
            "9\n",
            "10\n",
            "10\n",
            "8\n",
            "8\n",
            "7\n",
            "7\n",
            "7\n",
            "10\n",
            "9\n",
            "5\n",
            "7\n",
            "5\n",
            "9\n",
            "9\n",
            "10\n",
            "8\n",
            "10\n",
            "9\n",
            "9\n",
            "7\n",
            "9\n",
            "7\n",
            "5\n",
            "9\n",
            "9\n",
            "10\n",
            "10\n",
            "10\n",
            "9\n",
            "8\n",
            "10\n",
            "10\n",
            "10\n",
            "8\n",
            "7\n",
            "10\n",
            "8\n",
            "10\n",
            "9\n",
            "8\n",
            "10\n",
            "9\n",
            "9\n",
            "10\n",
            "9\n",
            "10\n",
            "9\n",
            "10\n",
            "10\n",
            "7\n",
            "9\n",
            "10\n",
            "9\n",
            "8\n",
            "10\n",
            "7\n",
            "9\n",
            "8\n",
            "9\n",
            "5\n",
            "10\n",
            "10\n",
            "7\n",
            "8\n",
            "10\n",
            "5\n",
            "9\n",
            "10\n",
            "9\n",
            "8\n",
            "10\n",
            "8\n",
            "10\n",
            "10\n",
            "10\n",
            "10\n",
            "5\n",
            "5\n",
            "10\n",
            "10\n",
            "10\n",
            "10\n",
            "10\n",
            "8\n",
            "10\n",
            "8\n",
            "8\n",
            "10\n",
            "8\n",
            "10\n",
            "8\n",
            "10\n",
            "10\n",
            "7\n",
            "10\n",
            "9\n",
            "10\n",
            "9\n",
            "9\n",
            "9\n",
            "9\n",
            "9\n",
            "8\n",
            "9\n",
            "8\n",
            "9\n",
            "10\n",
            "8\n",
            "8\n",
            "9\n",
            "9\n",
            "10\n",
            "10\n",
            "5\n",
            "5\n",
            "9\n",
            "8\n",
            "10\n",
            "8\n",
            "10\n",
            "9\n",
            "8\n",
            "9\n",
            "10\n",
            "10\n",
            "10\n",
            "10\n",
            "9\n",
            "5\n",
            "9\n",
            "8\n",
            "7\n",
            "4\n",
            "9\n",
            "9\n",
            "7\n",
            "8\n",
            "10\n",
            "8\n",
            "9\n",
            "8\n",
            "10\n",
            "9\n",
            "9\n",
            "8\n",
            "10\n",
            "10\n",
            "10\n",
            "10\n",
            "9\n",
            "9\n",
            "9\n",
            "8\n",
            "9\n",
            "3\n",
            "9\n",
            "10\n",
            "9\n",
            "9\n",
            "10\n",
            "8\n",
            "7\n",
            "9\n",
            "9\n",
            "9\n",
            "10\n",
            "9\n",
            "5\n",
            "10\n",
            "10\n",
            "8\n",
            "9\n",
            "8\n",
            "10\n",
            "8\n",
            "10\n",
            "10\n",
            "10\n",
            "10\n",
            "8\n",
            "5\n",
            "5\n",
            "2\n",
            "10\n",
            "8\n",
            "9\n",
            "8\n",
            "9\n",
            "9\n",
            "7\n",
            "1\n",
            "10\n",
            "8\n",
            "7\n",
            "10\n",
            "9\n",
            "10\n",
            "9\n",
            "10\n",
            "8\n",
            "9\n",
            "4\n",
            "10\n",
            "10\n",
            "10\n",
            "8\n",
            "8\n",
            "8\n",
            "5\n",
            "9\n",
            "7\n",
            "9\n",
            "9\n",
            "9\n",
            "10\n",
            "8\n",
            "10\n",
            "10\n",
            "10\n",
            "10\n",
            "10\n",
            "8\n",
            "10\n",
            "9\n",
            "10\n",
            "10\n",
            "10\n",
            "5\n",
            "9\n",
            "8\n",
            "7\n",
            "9\n",
            "8\n",
            "9\n",
            "8\n",
            "5\n",
            "10\n",
            "5\n",
            "10\n",
            "9\n",
            "10\n",
            "10\n",
            "10\n",
            "10\n",
            "10\n",
            "8\n",
            "10\n",
            "8\n",
            "5\n",
            "10\n",
            "10\n",
            "9\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "10\n",
            "9\n",
            "5\n",
            "8\n",
            "9\n",
            "7\n",
            "10\n",
            "8\n",
            "9\n",
            "10\n",
            "8\n",
            "9\n",
            "9\n",
            "8\n",
            "9\n",
            "8\n",
            "10\n",
            "8\n",
            "10\n",
            "8\n",
            "8\n",
            "10\n",
            "8\n",
            "8\n",
            "10\n",
            "10\n",
            "10\n",
            "9\n",
            "8\n",
            "8\n",
            "9\n",
            "10\n",
            "8\n",
            "10\n",
            "10\n",
            "9\n",
            "9\n",
            "4\n",
            "10\n",
            "10\n",
            "9\n",
            "10\n",
            "9\n",
            "9\n",
            "10\n",
            "9\n",
            "2\n",
            "9\n",
            "9\n",
            "8\n",
            "2\n",
            "10\n",
            "9\n",
            "8\n",
            "10\n",
            "10\n",
            "8\n",
            "8\n",
            "10\n",
            "9\n",
            "8\n",
            "9\n",
            "7\n",
            "8\n",
            "10\n",
            "10\n",
            "9\n",
            "7\n",
            "9\n",
            "9\n",
            "9\n",
            "9\n",
            "10\n",
            "8\n",
            "10\n",
            "8\n",
            "10\n",
            "10\n",
            "9\n",
            "7\n",
            "4\n",
            "8\n",
            "5\n",
            "9\n",
            "10\n",
            "8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpN6mHoGcAJE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}