{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn import linear_model\n",
    "from pymystem3 import Mystem\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('texts_train.txt', 'r') as file_texts:\n",
    "    texts = np.array(file_texts.read().strip(\"\\n\").split(\"\\n\"))\n",
    "file_texts.close()\n",
    "\n",
    "with open('scores_train.txt', 'r') as file_scores:\n",
    "    scores_str = np.array(file_scores.read().strip(\"\\n\").split(\"\\n\"))\n",
    "    scores = np.array([int(elem) for elem in scores_str if elem != ''])\n",
    "file_scores.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score is 7.8584\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean score is\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset_40757_1 (22).txt', 'r') as file_tests:\n",
    "    texts_test = file_tests.read().strip(\"\\n\").split(\"\\n\")\n",
    "file_tests.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['сериал',\n",
       " 'очень',\n",
       " 'люблю',\n",
       " 'но',\n",
       " 'академия',\n",
       " 'и',\n",
       " 'земля',\n",
       " 'вызывает',\n",
       " 'у',\n",
       " 'меня',\n",
       " 'отторжение',\n",
       " 'идеей',\n",
       " '...',\n",
       " 'не',\n",
       " 'люблю',\n",
       " 'когда',\n",
       " 'принижают',\n",
       " 'ценность',\n",
       " 'человека',\n",
       " 'как',\n",
       " 'личности',\n",
       " 'даже',\n",
       " 'не',\n",
       " 'смотря',\n",
       " 'на',\n",
       " 'ошибки',\n",
       " 'личности']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def maybe_to_lower(text):\n",
    "    match = re.search(\"\\w+\", text)\n",
    "    if not match:\n",
    "        return text\n",
    "    if text.isupper():\n",
    "        return text\n",
    "    return text.lower()\n",
    "\n",
    "def tokenize(text):\n",
    "    res = []\n",
    "    ind = 0\n",
    "    cur_seq = \"\"\n",
    "    if_word = True\n",
    "    \n",
    "    n = len(text)\n",
    "    stop_signs = set(['.', ',', '-', ';'])\n",
    "    \n",
    "    while True:\n",
    "        if ind >= n:\n",
    "           break \n",
    "        \n",
    "        match = re.search(\"\\w\", text[ind])\n",
    "        \n",
    "        if if_word:\n",
    "            if match:\n",
    "                cur_seq += text[ind]\n",
    "                ind += 1\n",
    "            else:\n",
    "                cur_seq = cur_seq.replace(' ', '')\n",
    "                if cur_seq != '' and cur_seq not in stop_signs:\n",
    "                    res.append(maybe_to_lower(cur_seq))\n",
    "                cur_seq = text[ind]\n",
    "                if_word = False\n",
    "                ind += 1\n",
    "        else:\n",
    "            if match:\n",
    "                cur_seq = cur_seq.replace(' ', '')\n",
    "                if cur_seq != '' and cur_seq not in stop_signs:\n",
    "                    res.append(maybe_to_lower(cur_seq))\n",
    "                cur_seq = text[ind]\n",
    "                if_word = True\n",
    "                ind += 1\n",
    "            else:\n",
    "                cur_seq += text[ind]\n",
    "                ind += 1\n",
    "            \n",
    "    return res\n",
    "            \n",
    "        \n",
    "tokenize(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef8215f777f4f0d97cd9be112f3077f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts = []\n",
    "\n",
    "for text in tqdm(texts):\n",
    "    tokenized_texts.append(tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['сериал',\n",
       "  'очень',\n",
       "  'люблю',\n",
       "  'но',\n",
       "  'академия',\n",
       "  'и',\n",
       "  'земля',\n",
       "  'вызывает',\n",
       "  'у',\n",
       "  'меня',\n",
       "  'отторжение',\n",
       "  'идеей',\n",
       "  '...',\n",
       "  'не',\n",
       "  'люблю',\n",
       "  'когда',\n",
       "  'принижают',\n",
       "  'ценность',\n",
       "  'человека',\n",
       "  'как',\n",
       "  'личности',\n",
       "  'даже',\n",
       "  'не',\n",
       "  'смотря',\n",
       "  'на',\n",
       "  'ошибки',\n",
       "  'личности'],\n",
       " ['думал',\n",
       "  'что',\n",
       "  'будет',\n",
       "  'лучше',\n",
       "  'идея',\n",
       "  'очень',\n",
       "  'интересна',\n",
       "  'города',\n",
       "  'иные',\n",
       "  '..',\n",
       "  'но',\n",
       "  'в',\n",
       "  'целом',\n",
       "  'чуть',\n",
       "  'выше',\n",
       "  'чем'],\n",
       " ['с',\n",
       "  'творчеством',\n",
       "  'головачева',\n",
       "  'я',\n",
       "  'познакомился',\n",
       "  'посредством',\n",
       "  'этой',\n",
       "  'книги',\n",
       "  'понравилось',\n",
       "  'дико',\n",
       "  'и',\n",
       "  'потом',\n",
       "  'было',\n",
       "  'жалко',\n",
       "  'что',\n",
       "  'остальные',\n",
       "  'произведения',\n",
       "  'подобного',\n",
       "  'жанра',\n",
       "  'ни',\n",
       "  'по',\n",
       "  'накалу',\n",
       "  'страстей',\n",
       "  'ни',\n",
       "  'по',\n",
       "  'сюжету',\n",
       "  'даже',\n",
       "  'блиско',\n",
       "  'не',\n",
       "  'подходили',\n",
       "  'посланник',\n",
       "  'мне',\n",
       "  'вообще',\n",
       "  'больше',\n",
       "  'всех',\n",
       "  'книг',\n",
       "  'понравился',\n",
       "  'у',\n",
       "  'головачева'],\n",
       " ['то',\n",
       "  'то',\n",
       "  'я',\n",
       "  'и',\n",
       "  'в',\n",
       "  'большое',\n",
       "  'неудовольствие',\n",
       "  'прочитал',\n",
       "  '\"',\n",
       "  'артура',\n",
       "  '\"..',\n",
       "  'теперь',\n",
       "  'понимаю',\n",
       "  'почему',\n",
       "  'стиль',\n",
       "  'на',\n",
       "  'асприновский'],\n",
       " ['как',\n",
       "  'мне',\n",
       "  'показалось',\n",
       "  'местами',\n",
       "  'сильно',\n",
       "  'смахивает',\n",
       "  'на',\n",
       "  'посланника',\n",
       "  'но',\n",
       "  'по',\n",
       "  'общим',\n",
       "  'впечатлениям',\n",
       "  'не',\n",
       "  'дотягивает',\n",
       "  'концовка',\n",
       "  '(',\n",
       "  'последняя',\n",
       "  'треть',\n",
       "  'книги',\n",
       "  ')',\n",
       "  'такое',\n",
       "  'ощущение',\n",
       "  'писалась',\n",
       "  'второпях',\n",
       "  'вобщем',\n",
       "  'почитать',\n",
       "  'можно',\n",
       "  'только',\n",
       "  'от',\n",
       "  'нечего'],\n",
       " ['от',\n",
       "  'первой',\n",
       "  'части',\n",
       "  'книги',\n",
       "  'просто',\n",
       "  'оторваться',\n",
       "  'не',\n",
       "  'мог',\n",
       "  'уж',\n",
       "  'больно',\n",
       "  'захватывающими',\n",
       "  'были',\n",
       "  'похождения',\n",
       "  'бывшего',\n",
       "  'афганца',\n",
       "  'по',\n",
       "  'древней',\n",
       "  'руси',\n",
       "  'от',\n",
       "  'второй',\n",
       "  'части',\n",
       "  'отрываться',\n",
       "  'мог',\n",
       "  'но',\n",
       "  'ненадолго'],\n",
       " ['читается', 'конечно', 'легко', '..', 'но', 'уж', 'очень', 'ощущение'],\n",
       " ['прочитал',\n",
       "  'на',\n",
       "  'одном',\n",
       "  'дыхании',\n",
       "  '!',\n",
       "  'очень',\n",
       "  'понравилось',\n",
       "  '!',\n",
       "  'даже',\n",
       "  'не',\n",
       "  'ожидал',\n",
       "  '!',\n",
       "  'несколько',\n",
       "  'портит',\n",
       "  'общее',\n",
       "  'впечатление',\n",
       "  'язык',\n",
       "  'автора',\n",
       "  'но',\n",
       "  'вроде',\n",
       "  'он',\n",
       "  'еще',\n",
       "  'только',\n",
       "  'начинающий',\n",
       "  'так',\n",
       "  'что',\n",
       "  'надеюсь',\n",
       "  'поправит',\n",
       "  '!',\n",
       "  'в',\n",
       "  'целом',\n",
       "  'книгу',\n",
       "  'рекомендую',\n",
       "  'но',\n",
       "  'это',\n",
       "  'только',\n",
       "  'начало',\n",
       "  'продолжение',\n",
       "  'нашел',\n",
       "  'в',\n",
       "  'сети',\n",
       "  'но',\n",
       "  'очень',\n",
       "  'оно',\n",
       "  'сырое'],\n",
       " ['дочитав',\n",
       "  'я',\n",
       "  'ещё',\n",
       "  'несколько',\n",
       "  'дней',\n",
       "  'не',\n",
       "  'могла',\n",
       "  'в',\n",
       "  'себя',\n",
       "  'придти',\n",
       "  'настолько',\n",
       "  'потрясло',\n",
       "  'то',\n",
       "  'как',\n",
       "  'она',\n",
       "  'заканчивается',\n",
       "  '!!!',\n",
       "  'еще',\n",
       "  'раз',\n",
       "  'мураками',\n",
       "  'написал',\n",
       "  'картину',\n",
       "  'которую',\n",
       "  'нельзя',\n",
       "  'потрогать',\n",
       "  'полностью',\n",
       "  'понять',\n",
       "  'но',\n",
       "  'ты',\n",
       "  'можешь',\n",
       "  'почувствовать',\n",
       "  'запах',\n",
       "  'ее',\n",
       "  'красок',\n",
       "  'и',\n",
       "  'ее',\n",
       "  'странное',\n",
       "  'и',\n",
       "  'доброе',\n",
       "  'настроение',\n",
       "  'это',\n",
       "  'больше',\n",
       "  'чем',\n",
       "  'жизнь',\n",
       "  'это',\n",
       "  'дальше',\n",
       "  'чем',\n",
       "  'смерть'],\n",
       " ['сериал', 'впечатлил', '!', 'интересная', 'идея', 'слов', 'силы']]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 20000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db781f0ceae349499c4c7ada31b6507b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word2scores = {}\n",
    "\n",
    "num_texts = len(texts)\n",
    "num_scores = len(scores)\n",
    "\n",
    "print(num_texts, num_scores)\n",
    "\n",
    "for i in tqdm(range(num_texts)):\n",
    "    tokenized = tokenized_texts[i]\n",
    "    tokens_set = set(tokenized)\n",
    "    score = scores[i]\n",
    "    \n",
    "    for token in tokens_set:\n",
    "        if token not in word2scores:\n",
    "            word2scores[token] = []\n",
    "        word2scores[token].append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2score = {elem : np.array(word2scores[elem]).mean() for elem in word2scores.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.071428571428571"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2score[':(']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_file = open(\"res.txt\", \"w\")\n",
    "res = \"\"\n",
    "\n",
    "for text in texts_test:\n",
    "    tokenized = tokenize(text)\n",
    "    text_scores = np.array([word2score[elem] for elem in tokenized if elem in word2score])\n",
    "    print(text_scores)\n",
    "    if len(text_scores) == 0:\n",
    "        res += \"8\\n\"\n",
    "    else:\n",
    "        res += str(int(round(text_scores.mean()))) + \"\\n\"\n",
    "    \n",
    "res_file.write(res)\n",
    "res_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['сериал',\n",
       " 'очень',\n",
       " 'люблю',\n",
       " 'но',\n",
       " 'академия',\n",
       " 'и',\n",
       " 'земля',\n",
       " 'вызывает',\n",
       " 'у',\n",
       " 'меня',\n",
       " 'отторжение',\n",
       " 'идеей',\n",
       " 'не',\n",
       " 'люблю',\n",
       " 'когда',\n",
       " 'принижают',\n",
       " 'ценность',\n",
       " 'человека',\n",
       " 'как',\n",
       " 'личности',\n",
       " 'даже',\n",
       " 'не',\n",
       " 'смотря',\n",
       " 'на',\n",
       " 'ошибки',\n",
       " 'личности']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_to_words(text):\n",
    "    words = re.split(\"\\W+\", text)\n",
    "    res = [elem.lower() for elem in words if elem != '']\n",
    "    return res\n",
    "        \n",
    "tokenize_to_words(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 20000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca7c92135ac4610875939d6095e6de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word2scores = {}\n",
    "\n",
    "num_texts = len(texts)\n",
    "num_scores = len(scores)\n",
    "\n",
    "print(num_texts, num_scores)\n",
    "\n",
    "for i in tqdm(range(num_texts)):\n",
    "    tokenized = tokenized_texts[i]\n",
    "    tokens_set = set(tokenized)\n",
    "    score = scores[i]\n",
    "    \n",
    "    for token in tokens_set:\n",
    "        if token not in word2scores:\n",
    "            word2scores[token] = []\n",
    "        word2scores[token].append(score)\n",
    "        \n",
    "word2score = {elem : np.array(word2scores[elem]).mean() for elem in word2scores.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_file = open(\"res.txt\", \"w\")\n",
    "res = \"\"\n",
    "\n",
    "for text in texts_test:\n",
    "    tokenized = tokenize(text)\n",
    "    text_scores = np.array([word2score[elem] for elem in tokenized if elem in word2score])\n",
    "    print(text_scores)\n",
    "    if len(text_scores) == 0:\n",
    "        res += \"8\\n\"\n",
    "    else:\n",
    "        res += str(int(round(text_scores.mean()))) + \"\\n\"\n",
    "    \n",
    "res_file.write(res)\n",
    "res_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdcafabc8d324f4084c0306a54896b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['сериал',\n",
       "  'очень',\n",
       "  'любить',\n",
       "  'но',\n",
       "  'академия',\n",
       "  'и',\n",
       "  'земля',\n",
       "  'вызывать',\n",
       "  'у',\n",
       "  'я',\n",
       "  'отторжение',\n",
       "  'идея',\n",
       "  '...',\n",
       "  'не',\n",
       "  'любить',\n",
       "  'когда',\n",
       "  'принижать',\n",
       "  'ценность',\n",
       "  'человек',\n",
       "  'как',\n",
       "  'личность',\n",
       "  'даже',\n",
       "  'не',\n",
       "  'смотря',\n",
       "  'на',\n",
       "  'ошибка',\n",
       "  'личность'],\n",
       " ['думать',\n",
       "  'что',\n",
       "  'быть',\n",
       "  'хорошо',\n",
       "  'идея',\n",
       "  'очень',\n",
       "  'интересный',\n",
       "  'город',\n",
       "  'иной',\n",
       "  '..',\n",
       "  'но',\n",
       "  'в',\n",
       "  'целое',\n",
       "  'чуть',\n",
       "  'высоко',\n",
       "  'что'],\n",
       " ['с',\n",
       "  'творчество',\n",
       "  'головачев',\n",
       "  'я',\n",
       "  'познакомиться',\n",
       "  'посредством',\n",
       "  'этот',\n",
       "  'книга',\n",
       "  'понравиться',\n",
       "  'дико',\n",
       "  'и',\n",
       "  'потом',\n",
       "  'быть',\n",
       "  'жалко',\n",
       "  'что',\n",
       "  'остальной',\n",
       "  'произведение',\n",
       "  'подобный',\n",
       "  'жанр',\n",
       "  'ни',\n",
       "  'по',\n",
       "  'накал',\n",
       "  'страсть',\n",
       "  'ни',\n",
       "  'по',\n",
       "  'сюжет',\n",
       "  'даже',\n",
       "  'блиско',\n",
       "  'не',\n",
       "  'подходить',\n",
       "  'посланник',\n",
       "  'я',\n",
       "  'вообще',\n",
       "  'много',\n",
       "  'все',\n",
       "  'книга',\n",
       "  'понравиться',\n",
       "  'у',\n",
       "  'головачев'],\n",
       " ['то',\n",
       "  'то',\n",
       "  'я',\n",
       "  'и',\n",
       "  'в',\n",
       "  'большой',\n",
       "  'неудовольствие',\n",
       "  'прочитывать',\n",
       "  ' \" ',\n",
       "  'артур',\n",
       "  ' \"',\n",
       "  '..',\n",
       "  'теперь',\n",
       "  'понимать',\n",
       "  'почему',\n",
       "  'стиль',\n",
       "  'на',\n",
       "  'асприновский'],\n",
       " ['как',\n",
       "  'я',\n",
       "  'показываться',\n",
       "  'место',\n",
       "  'сильно',\n",
       "  'смахивать',\n",
       "  'на',\n",
       "  'посланник',\n",
       "  'но',\n",
       "  'по',\n",
       "  'общий',\n",
       "  'впечатление',\n",
       "  'не',\n",
       "  'дотягивать',\n",
       "  'концовка',\n",
       "  ' ( ',\n",
       "  'последний',\n",
       "  'треть',\n",
       "  'книга',\n",
       "  ' ) ',\n",
       "  'такой',\n",
       "  'ощущение',\n",
       "  'писаться',\n",
       "  'второпях',\n",
       "  'вобщий',\n",
       "  'почитать',\n",
       "  'можно',\n",
       "  'только',\n",
       "  'от',\n",
       "  'нечего'],\n",
       " ['от',\n",
       "  'первый',\n",
       "  'часть',\n",
       "  'книга',\n",
       "  'просто',\n",
       "  'отрываться',\n",
       "  'не',\n",
       "  'мочь',\n",
       "  'уж',\n",
       "  'больно',\n",
       "  'захватывать',\n",
       "  'быть',\n",
       "  'похождение',\n",
       "  'бывший',\n",
       "  'афганец',\n",
       "  'по',\n",
       "  'древний',\n",
       "  'русь',\n",
       "  'от',\n",
       "  'второй',\n",
       "  'часть',\n",
       "  'отрываться',\n",
       "  'мочь',\n",
       "  'но',\n",
       "  'ненадолго'],\n",
       " ['читаться', 'конечно', 'легко', '..', 'но', 'уж', 'очень', 'ощущение'],\n",
       " ['прочитывать',\n",
       "  'на',\n",
       "  'один',\n",
       "  'дыхание',\n",
       "  '!',\n",
       "  'очень',\n",
       "  'понравиться',\n",
       "  '!',\n",
       "  'даже',\n",
       "  'не',\n",
       "  'ожидать',\n",
       "  '!',\n",
       "  'несколько',\n",
       "  'портить',\n",
       "  'общий',\n",
       "  'впечатление',\n",
       "  'язык',\n",
       "  'автор',\n",
       "  'но',\n",
       "  'вроде',\n",
       "  'он',\n",
       "  'еще',\n",
       "  'только',\n",
       "  'начинать',\n",
       "  'так',\n",
       "  'что',\n",
       "  'надеяться',\n",
       "  'поправлять',\n",
       "  '!',\n",
       "  'в',\n",
       "  'целое',\n",
       "  'книга',\n",
       "  'рекомендовать',\n",
       "  'но',\n",
       "  'это',\n",
       "  'только',\n",
       "  'начало',\n",
       "  'продолжение',\n",
       "  'находить',\n",
       "  'в',\n",
       "  'сеть',\n",
       "  'но',\n",
       "  'очень',\n",
       "  'оно',\n",
       "  'сырой'],\n",
       " ['дочитывать',\n",
       "  'я',\n",
       "  'еще',\n",
       "  'несколько',\n",
       "  'день',\n",
       "  'не',\n",
       "  'мочь',\n",
       "  'в',\n",
       "  'себя',\n",
       "  'прийти',\n",
       "  'настолько',\n",
       "  'потрясать',\n",
       "  'то',\n",
       "  'как',\n",
       "  'она',\n",
       "  'заканчиваться',\n",
       "  '!!!',\n",
       "  'еще',\n",
       "  'раз',\n",
       "  'мурак',\n",
       "  'написать',\n",
       "  'картина',\n",
       "  'который',\n",
       "  'нельзя',\n",
       "  'потрогать',\n",
       "  'полностью',\n",
       "  'понимать',\n",
       "  'но',\n",
       "  'ты',\n",
       "  'мочь',\n",
       "  'почувствовать',\n",
       "  'запах',\n",
       "  'ее',\n",
       "  'краска',\n",
       "  'и',\n",
       "  'ее',\n",
       "  'странный',\n",
       "  'и',\n",
       "  'добрый',\n",
       "  'настроение',\n",
       "  'это',\n",
       "  'большой',\n",
       "  'чем',\n",
       "  'жизнь',\n",
       "  'это',\n",
       "  'далеко',\n",
       "  'чем',\n",
       "  'смерть'],\n",
       " ['сериал', 'впечатлить', '!', 'интересный', 'идея', 'слово', 'сила']]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_texts = []\n",
    "\n",
    "for text in tqdm(texts):\n",
    "    tokenized = tokenize(text)\n",
    "    text_ = \" \".join(tokenized)\n",
    "    lemmatized = mystem.lemmatize(text_)\n",
    "    res = [elem for elem in lemmatized if elem != ' ' and elem != '\\n']\n",
    "    tokenized_texts.append(res)\n",
    "\n",
    "tokenized_texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 20000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "206dfd3b407e48b6870a0caf77c202da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word2scores = {}\n",
    "\n",
    "num_texts = len(texts)\n",
    "num_scores = len(scores)\n",
    "\n",
    "print(num_texts, num_scores)\n",
    "\n",
    "for i in tqdm(range(num_texts)):\n",
    "    tokenized = tokenized_texts[i]\n",
    "    tokens_set = set(tokenized)\n",
    "    score = scores[i]\n",
    "    \n",
    "    for token in tokens_set:\n",
    "        if token not in word2scores:\n",
    "            word2scores[token] = []\n",
    "        word2scores[token].append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_file = open(\"res.txt\", \"w\")\n",
    "res = \"\"\n",
    "\n",
    "for text in texts_test:\n",
    "    tokenized = tokenize(text)\n",
    "    text_ = \" \".join(tokenized)\n",
    "    lemmatized = mystem.lemmatize(text_)\n",
    "    result = [elem for elem in lemmatized if elem != ' ']\n",
    "    \n",
    "    text_scores = np.array([word2score[elem] for elem in result if elem in word2score])\n",
    "    #print(text_scores)\n",
    "    if len(text_scores) == 0:\n",
    "        res += \"8\\n\"\n",
    "    else:\n",
    "        res += str(int(round(text_scores.mean()))) + \"\\n\"\n",
    "    \n",
    "print(res)\n",
    "res_file.write(res)\n",
    "res_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_texts = [\" \".join(elem) for elem in tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['сериал очень любить но академия и земля вызывать у я отторжение идея ... не любить когда принижать ценность человек как личность даже не смотря на ошибка личность',\n",
       " 'думать что быть хорошо идея очень интересный город иной .. но в целое чуть высоко что',\n",
       " 'с творчество головачев я познакомиться посредством этот книга понравиться дико и потом быть жалко что остальной произведение подобный жанр ни по накал страсть ни по сюжет даже блиско не подходить посланник я вообще много все книга понравиться у головачев',\n",
       " 'то то я и в большой неудовольствие прочитывать  \"  артур  \" .. теперь понимать почему стиль на асприновский',\n",
       " 'как я показываться место сильно смахивать на посланник но по общий впечатление не дотягивать концовка  (  последний треть книга  )  такой ощущение писаться второпях вобщий почитать можно только от нечего',\n",
       " 'от первый часть книга просто отрываться не мочь уж больно захватывать быть похождение бывший афганец по древний русь от второй часть отрываться мочь но ненадолго',\n",
       " 'читаться конечно легко .. но уж очень ощущение',\n",
       " 'прочитывать на один дыхание ! очень понравиться ! даже не ожидать ! несколько портить общий впечатление язык автор но вроде он еще только начинать так что надеяться поправлять ! в целое книга рекомендовать но это только начало продолжение находить в сеть но очень оно сырой',\n",
       " 'дочитывать я еще несколько день не мочь в себя прийти настолько потрясать то как она заканчиваться !!! еще раз мурак написать картина который нельзя потрогать полностью понимать но ты мочь почувствовать запах ее краска и ее странный и добрый настроение это большой чем жизнь это далеко чем смерть',\n",
       " 'сериал впечатлить ! интересный идея слово сила']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = []\n",
    "\n",
    "for text in texts_test:\n",
    "    tokenized = tokenize(text)\n",
    "    text_ = \" \".join(tokenized)\n",
    "    lemmatized = mystem.lemmatize(text_)\n",
    "    result = [elem for elem in lemmatized if elem != ' ']\n",
    "    result = \" \".join(result)\n",
    "    test_dataset.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(  29  /  12  /  07  )  легко и непринужденный правда признаваться книга я не читать .. я она слушать  ) . конец и правда разочаровывать но в общий очень понравиться ! не поклонник мурак но кой что из написать увлекать и подарить ощущение который на тот момент не находить у другой писатель хотя сейчас мочь выделять несколько ... читать переживать все в себя если книга действительно стоить внимание для конкретный личность то она и быть подобный переживать приключение если же нет книга просто не ваш \\n',\n",
       " 'потрясно !  я обожать этот книга \\n',\n",
       " 'да весь очень знакомый когда все раздражать хотеться близкий человек сделать назло больно это переходный возраст у ребенок а вот мать возможно действительно болезнь альцгеймер конец на мой взгляд немного недосказывать саундтрек просто супер \\n',\n",
       " 'созвучно \\n',\n",
       " 'книга о любовь реальный и виртуальный главный герой встречаться в аська страсть накаляться вот они уже собираться встречаться в реальный жизнь что же это заканчиваться ? ну без бурный секс наверное не обходиться а что потом ? книга еще не дочитывать но думать что по логика вещь концовка быть печальный реальность от виртуальность во многое отличаться увы в аська мы пора убегать от свой проблема и искать там то что мы не доставать в действительность думать тем у кто быть какой то виртуальный отношение книга должный понравиться хотя отзыв здесь противоречивый дочитывать до конец весь таки она  (  гл героиня  )  поступать очень \\n',\n",
       " 'наверно еще не дорастать  ))  в школа осиливать с большиииим труд хотя булгаков просто обожать ! пока желание перечитывать нет \\n',\n",
       " '\"  запредельный усилие  \"  все также присутствовать но уже замаскировывать можно читать без раздражение сюжет все еще интересный но как и рано не \\n',\n",
       " 'заинтересовывать увлекать утомлять разочаровывать это если кратко описывать процесс прочтение книга а если по русски  -\"  начинать за здравие кончать за упокой  \" . начало уверять завязка интересный персонаж живой и не тривиальный сюжет вроде как не линейный несколько относительно не связанный линия так первый треть книга поставлять бы уверенный 9 а вот далеко ... как то грустно становиться нет не от содержание а от интрига она как то незаметно скончаться в уголочек один из рецензент упоминать браун позволять себя вносить дополнение плохой браун за такой относительно длинный роман развитие характер так и не происходить действие идти себя и идти и вообщий то никто не волновать вроде как момент даже и стрелять а вот драйв это не приносить а концовка это вообще тихий ужас  ((( . такой хэппи энд не во весь мелодрама находить вывод  :  не надо ждать откровение и экстаз простенький детектив в скандинавский антураж читать продолжение не тянуть абсолютно P S вот за что хотеть бы сказать отдельный спасибо за момент отбытие тюремный наказание аж захотеться весь бросать и посидеть месяц другой в такой вот скандинавский тюрьма отдыхать так сказать душа и тело \\n',\n",
       " 'в свой время читать с огромный удовольствие ! сейчас конечно взгляд на жизнь поменяться и герой мурак уже не столь близкий но все равный американизированый японец свой дело знать ! творчество завораживать и притягивать к себя так что не отрываться но на любитель опять же \\n',\n",
       " 'роман  :  нквд  :  война сневедомый автор  :  александр бушков издательство  :  ом пресс2004 жанр  :  о что  :  а я этот книжка понравиться настоящий будни чекист до вести отечествена и во время она и после как они вести розыск бандит предатель и во время розыск натыкаться на неведомый и как с это борливаться ни какой мультяшность от  \"  охотник за приведение  \" . порй и сам начинаеш вспоминать а точно я кто то это расказывать гдето я это читать или это со я быть мой оценка  :  5 \\n']"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(binary=True, ngram_range=(2, 3))\n",
    "train_dataset = vectorizer.fit_transform(final_texts)\n",
    "\n",
    "model = linear_model.SGDClassifier().fit(train_dataset, scores)\n",
    "\n",
    "res_file = open(\"res.txt\", \"w\")\n",
    "res = \"\"\n",
    "\n",
    "for text in test_dataset:\n",
    "    vectorized = vectorizer.transform([text])\n",
    "    unclipped_res = int(round(model.predict(vectorized)[0]))\n",
    "    \n",
    "    res += str(np.clip(unclipped_res, 1, 10)) + \"\\n\"\n",
    "\n",
    "print(res)\n",
    "res_file.write(res)\n",
    "res_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2))\n",
    "train_dataset = vectorizer.fit_transform(final_texts)\n",
    "\n",
    "model = linear_model.SGDRegressor().fit(train_dataset, scores)\n",
    "\n",
    "res_file = open(\"res.txt\", \"w\")\n",
    "res = \"\"\n",
    "\n",
    "for text in test_dataset:\n",
    "    vectorized = vectorizer.transform([text])\n",
    "    unclipped_res = int(round(model.predict(vectorized)[0]))\n",
    "    \n",
    "    res += str(np.clip(unclipped_res, 1, 10)) + \"\\n\"\n",
    "\n",
    "print(res)\n",
    "res_file.write(res)\n",
    "res_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "train_dataset = vectorizer.fit_transform(final_texts)\n",
    "\n",
    "model = linear_model.PassiveAggressiveRegressor().fit(train_dataset, scores)\n",
    "\n",
    "res_file = open(\"res.txt\", \"w\")\n",
    "res = \"\"\n",
    "\n",
    "for text in test_dataset:\n",
    "    vectorized = vectorizer.transform([text])\n",
    "    unclipped_res = int(round(model.predict(vectorized)[0]))\n",
    "    \n",
    "    res += str(np.clip(unclipped_res, 1, 10)) + \"\\n\"\n",
    "\n",
    "print(res)\n",
    "res_file.write(res)\n",
    "res_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = []\n",
    "\n",
    "for text in texts_test:\n",
    "    tokenized = tokenize(text)\n",
    "    text_ = \" \".join(tokenized)\n",
    "    lemmatized = mystem.lemmatize(text_)\n",
    "    result = [elem for elem in lemmatized if elem != ' ']\n",
    "    result = \" \".join(result)\n",
    "    test_dataset.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "train_dataset = vectorizer.fit_transform(final_texts)\n",
    "\n",
    "model = linear_model.SGDClassifier().fit(train_dataset, scores)\n",
    "\n",
    "res_file = open(\"res.txt\", \"w\")\n",
    "res = \"\"\n",
    "\n",
    "for text in test_dataset:\n",
    "    vectorized = vectorizer.transform([text])\n",
    "    unclipped_res = int(round(model.predict(vectorized)[0]))\n",
    "    \n",
    "    res += str(np.clip(unclipped_res, 1, 10)) + \"\\n\"\n",
    "\n",
    "print(res)\n",
    "res_file.write(res)\n",
    "res_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "train_dataset = vectorizer.fit_transform(final_texts)\n",
    "\n",
    "model = linear_model.Ridge().fit(train_dataset, scores)\n",
    "\n",
    "res_file = open(\"res.txt\", \"w\")\n",
    "res = \"\"\n",
    "\n",
    "for text in test_dataset:\n",
    "    vectorized = vectorizer.transform([text])\n",
    "    unclipped_res = int(round(model.predict(vectorized)[0]))\n",
    "    \n",
    "    res += str(np.clip(unclipped_res, 1, 10)) + \"\\n\"\n",
    "\n",
    "print(res)\n",
    "res_file.write(res)\n",
    "res_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
